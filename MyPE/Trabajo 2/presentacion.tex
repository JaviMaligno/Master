\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usetheme{Berlin}
\usepackage[spanish]{babel}
\usepackage{multirow}
%\usepackage{estilo-apuntes}
\usepackage[]{graphicx}
\usepackage{svg}
\usepackage{multicol}
\theoremstyle{definition}

\newtheorem{teorema}{Teorema}
\newtheorem{defi}{Definición}
\newtheorem{lema}{Lema}
\newtheorem{prop}{Proposición}
\newtheorem{coro}{Corolario}
\newtheorem{ejem}{Ejemplo}
\newtheorem{dem}{Demostración}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\conv}[1]{\overset{#1}{\longrightarrow}}
\providecommand{\convcs}{\xrightarrow{CS}}
\providecommand{\gene}[1]{\langle{#1}\rangle}
\providecommand{\posi}[1]{\left[#1\right]^+}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercolor{footline}{fg=black}
\setbeamerfont{footline}{series=\bfseries}


\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string
\definecolor{majo}{HTML}{CD2626}
\newcommand*{\ColorIfNotInString}[1]{\iftoggle{InString}{#1}{\color{majo}#1}}%
\newcommand*{\ProcessQuote}[1]{#1\iftoggle{InString}{\global\togglefalse{InString}}{\global\toggletrue{InString}}}%


% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{11}  % for normal

\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
}

%-----------------------------------------------------------

\title{Modelado y Predicción Estadística\\
Estimación núcleo de la curva de \\regresión mediante k-NN}
\author{Rafael González López 
}

\institute{
Universidad de Sevilla}
\date{}
 
\begin{document}
\frame{\titlepage}

\begin{frame}
\frametitle{Tabla de contenidos}
\tableofcontents
\end{frame}

\section{Introducción}

\begin{frame}
\frametitle{Introducción}
\begin{itemize}
\item Sea $(X,Y)$ un vector aleatorio continuo con función de densidad $f(x,y)$ y sea $f_X(x)$ la fdd marginal de $X$. Sabemos que el mejor predictor de $Y$ conocido $X$, en sentido de mínimos cuadrados, es la curva de regresión
$$
m(x)=E[Y\mid X=x]
$$
\item \textbf{Objetivo}. Estimar la curva de regresión a partir de una muestra aleatoria.
\item \textbf{Enfoque}. Utilizaremos el método no paramétrico conocido como $k$-\textit{nearest neighbour} o $k$-vecino más cercano.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Diferencias con el estimador núcleo}
\begin{itemize}
\item El estimador núcleo estaba definido como la media ponderada de los valores de la variable $Y$. Estas ponderaciones son dependientes del valor del parámetro ventana $h$ y naturalmente de la variable predictora $X$.
\item El estimador k-NN también es una media ponderada de los valores de $Y$. En este caso, la ponderación depende de ciertos conjuntos denominados entorno (\textit{neighborhood}). 
\item Estos a su vez están definidos mediante los $k$-vecinos más cercanos usando la distancia euclídea.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{k-NN}
\begin{defi}
Sea $k$ un entero positivo. Sea $X_1,\dotsc,X_n$ y sea $x$. Diremos que $X_{i_1},\dotsc,X_{i_k}$ son los $k$-vecinos más cercanos a $x$ si son los más próximos, en el sentido dado por la distancia euclídea, a $x$ entre los $X_1,\dotsc,X_n$.
\end{defi}
\end{frame}

\begin{frame}
\frametitle{Ejemplo}
\begin{ejem}
Sea $\{X_i\}_{i=1}^6 = \{2,4,5,5.5,7,8\}$, $x=4$ y $k=3$. En ese caso los $3$-vecinos más cercanos son
$$
\{2,4,5\}
$$
y la menor distancia de $x$ al conjunto es $1$.
\end{ejem}
\end{frame}

\section{Estimador k-NN}
\begin{frame}
\frametitle{Un posible estimador}
Sea una muestra aleatoria de tamaño $n$ para el vector aleatorio $(X,Y)$ y sea $k$ entero positivo. El estimador que proponemos para la curva de regresión $m(x)$ es
$$
\hat{m}(x)=\frac{1}{n}\sum_{i=1}^n W_{k,i}(x)Y_i
$$
donde 
$$
W_{k,i}(x) = \begin{cases}
\frac{n}{k} & \text{si $X_i$ es uno de los k-NN de $x$}\\
0 & \text{caso contrario}
\end{cases}
$$
\end{frame}

\begin{frame}
\frametitle{Acerca del estimador}
\begin{itemize}
\item El parámetro $k$ actúa regulando la suavidad del estimador. Por tanto, cumple un papel análogo al parámetro ventana del estimador núcleo.
\item Si pensásemos en un caso donde $n$ sea fijo y $k$ sea mayor que $n$, entonces simplemente tendremos la suma de las variables respuestas divida por $k$.
\item En general, para evitar ruido el parámetro de suavidad $k$ se elige como función de $n$ o incluso en función de los datos. Además, con el fin de reducir el sesgo, suele tomarse $k$ como función de $n$ de manera que $k_n/n\to 0$.
\item En general, no podremos alcanzar simultáneamente ambos objetivos.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Alternativas dentro del k-NN}
Stone sugirió utilizar otros pesos de manera alternativa a los $W_{i,k}$, a saber
$$
W_{i,R}(x)=\frac{1}{\hat{f}(x)}K\left(\frac{x-X_i}{R_n(x)}\right)
$$

donde $R_n(x)$ es la distancia euclídea entre $x$ y el $k$-ésimo vecino más cercano suyo entre los $X_i$, $K(\cdot)$ es una función núcleo y $\hat{f}(x)$ 
$$
\hat{f}(x) = \frac{1}{nR_n(x)}\sum_{i=1}^n K\left(\frac{x-X_i}{R_n(x)}\right)
$$
es un estimador de la función de densidad basado en el k-NN, cuyos detalles pueden consultarse en el apéndice de este mismo trabajo. 
\end{frame}

\begin{frame}
\begin{teorema}[Lai, 1997] 
Sea $k\to \infty$, $k/n\to 0$ y $n\to \infty$. Entonces el sesgo y la varianza del estimador k-NN $\hat{m}_k$ que hemos definido vienen dados por
\begin{align*}
E[\hat{m}_k(x)]-m(x) &\approx \frac{m''(x)f(x)+2m'(x)f(x)}{24f(x)^3}\left(\frac{k}{n}\right)^2\\
Var[\hat{m}_k(x)] &\approx \frac{\sigma^2(x)}{k}
\end{align*}

\end{teorema}
\end{frame}


\begin{frame}
\frametitle{Formulación general}
De manera más general podemos definir el estimador de la siguiente forma
$$
\hat{m}(x)=\sum_{i=1}^n W_{n,i} Y_i
$$
donde $(W_{n,1},\dotsc,W_{n,n})$ es un vector (posiblemente aleatorio) de probabilidad. 
\end{frame}

\begin{frame}
\frametitle{Notas}
\begin{itemize}
\item Si tomamos $W_{n,i} = 1/k$ si $X_i$ es uno de los $k$-vecinos más cercanos y $0$ en otro caso, tenemos el primer estimador propuesto.
\item Tomando los pesos como en la diapositiva $9$, es claro que esta formulación generaliza la estimación núcleo del $k$-NN.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Asunciones para los resultados}
Para probar algunos enunciados necesitamos algunas hipótesis adicionales.
\begin{enumerate}
\item La función $m(\cdot)$ es acotada y lipschitziana en $x$, es decir, $||m(x)-m(x')|| \leq M||x-x'||^\alpha$.
\item Supongamos que $\sum_{i=k+1}^n W_{n,i} = O(b_n)$. Asumimos que $b_n$ y $||W_n||_2$ convergen casi seguro a $0$. Además, $k/n\to 0$ y $k/\log n \to \infty$. 
\item $E[|Y-m(X)|^r]<\infty$ para algún $r>2$.
\end{enumerate}
Además, los resultados están en términos de una cantidad 
$$
\phi(h) = P[|x-x'|<h]
$$
\end{frame}

\begin{frame}
\begin{teorema}
Supongamos que se dan las asunciones anteriores y además
$$
\sum_{n=1}^\infty (\log n)^{(r-2)/2}\left(\frac{||W_n||_r}{||W_n||_2}\right)^r < \infty
$$
Entonces, de manera casi segura
$$
||\hat{m}(x)-m(x)|| = O(b_n + [\phi^{-1}(2k/n)]^\alpha + (\log n)^{1/2}||W_n||_2)
$$
donde
$$
\phi^{-1}(x):= \inf\{h \mid \phi(h)\geq x\}%%
$$\end{teorema}
\end{frame}


\begin{frame}

Comenzamos asumiendo un pequeño lema técnico.
\begin{lema}
Supongamos que $k/n\to 0$ y $k/\log n \to \infty$. Sea $H(x)$ la distancia entre $x$ y el $k$-ésimo vecino más cercano. Entonces de manera casi segura
$$
P[H\geq \phi^{-1}(2k/n), i.o.] \to 0
$$
donde i.o. quiere decir infinitas veces.
\end{lema}

\end{frame}

\begin{frame}
Pasemos a la demostración del teorema.
\begin{dem}
Comenzamos separando el error en el sesgo y la varianza. Si $Y=m(X)+\varepsilon$ entonces
$$
\norm{m(x)-\hat{m}(x)} \leq \norm{\sum_i W_{n,i}(m(X_i)-m(x))} + \norm{\sum_i W_{i,n} \varepsilon_i}
$$
Usando la Asunción 1 y el Lema anterior
\begin{align*}
\norm{\sum_i W_{n,i}(m(X_i)-m(x))}  & \leq 2B\sum_{i=k+1}^n + \norm{\sum_{i=1}^k W_{n,i}(m(X_i)-m(x))}\\
&= O(b_n + [\phi^{-1}(2k/n)]^\alpha)
\end{align*}

\end{dem}
\end{frame}

\begin{frame}
\begin{dem}
Vamos ahora con el término de la varianza.
\end{dem}
\end{frame}
\section{Aplicación computacional}

\begin{frame}
\frametitle{El paquete FNN}
Aunque la librería CLASS es popular dentro del análisis de datos para utilizar KNN, la función \textit{knn} de este paquete sirve solo para clasificación. En su lugar utilizaremos la función \textit{knn.reg} de la librería FNN. 
\newline
\\
\textcolor{red}{knn.reg(train, test = NULL, y, k =3 , \\ \qquad algorithm=c("kd\_ tree", 
''cover\_ tree", "brute"))}
\newline
\\

\end{frame}

\begin{frame}
\frametitle{El paquete FNN}

Los parámetros de esta función son \textit{train}, la matriz o data.frame del conjunto de entrenamiento; \textit{test}, el conjunto test si lo hubiera; \textit{k}, el número de vecinos a considerar; y \textit{algorithm}, el algoritmo para el cálculo de los vecinos más cercanos.
\newline
\\
De cara a realizar una validación cruzada para elegir el parámetro $k$, el objeto creado por \textit{knn.reg} contiene varias componentes.  Entre ellas, \textit{PRESS} es la suma de los errores al cuadrado (siempre que no se haya introducido nada en \textit{test}).

\end{frame}
\begin{frame}
\frametitle{Ejemplo: el conjunto de datos \textit{cars}}
Utilizaremos el conjunto de datos \textit{cars} que trae R. Estos son los datos de la velocidad y las distancia que les demoró pararse a 50 coches en 1920. Por tanto, tenemos dos variables \textit{speed} y \textit{dist}.
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & speed & dist \\ 
  \hline
1 & 4.00 & 2.00 \\ 
  2 & 4.00 & 10.00 \\ 
  3 & 7.00 & 4.00 \\ 
  4 & 7.00 & 22.00 \\ 
  5 & 8.00 & 16.00 \\ 
  6 & 9.00 & 10.00 \\ 
   \hline
\end{tabular}
\end{table}

\end{frame}


\begin{frame}
\frametitle{Representación de los datos}
\begin{figure}[h!]
\includegraphics[scale=0.5]{myfile}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{Representación de \textit{PRESS} en función de $k$}
\begin{figure}[h!]
\includegraphics[scale=0.5]{myfile2}
\end{figure}
\end{frame}

\begin{frame}
\begin{itemize}
\item Parece claro que debemos tomar $k=2$ en nuestro modelo.
\item El mínimo se alcanza para $8926.750$.
\item Si representamos el conjunto de datos y superponemos los datos predichos por el modelo como un plot lineal.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Conjunto de datos frente a curva de regresión obtenida}
\begin{figure}[h!]
\includegraphics[scale=0.5]{myfile3}
\end{figure}
\end{frame}

\section{Apéndice}

\begin{frame}
\begin{center}
\huge{Apéndice. Estimador de la función de densidad mediante métodos de k-vecinos más cercanos}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Resultados anteriores}
\begin{teorema}
Supongamos que $X_1,\dotsc,X_n$ son v.a. continuas con i.i.d fdd $f$ donde $f\in \mathcal{F}$, la familia de todas las funciones de densidad continuas definidas sobre $\R$. Entonces $\not\exists\hat{f}$ tal que $\hat{f}\in \mathcal{F}$ y 
$$
E_f[\hat{f}(x)]=f(x) \quad \forall x \in \R, \quad \forall f \in\mathcal{F}
$$
\end{teorema}
\end{frame}



\begin{frame}
\frametitle{Resultados anteriores}

\begin{teorema}[Glivenko-Cantelli]
Supongamos que $X_1,\dotsc,X_n$ son v.a. continuas con i.i.d con FdD $F(x)$. Sea además $F_n(x)$ la función de distribución empírica asociada a la muestra. Entonces
$$
\sup_{x\in \R}\lvert F_n(x)-F(x)\rvert \overset{c.s.}{\longrightarrow} 0
$$
\end{teorema}
\end{frame}

\begin{frame}
\frametitle{Estimador k-NN de la fdd}
Sean $X_1,\dotsc,X_n$ v.a. i.i.d. con función de densidad continua y acotada $f(x)$. Consideramos entonces el estimador de $f(x)$ dado por
$$
\hat{f}(x) = \frac{1}{nR_n(x)}\sum_{i=1}^n K\left(\frac{x-X_i}{R_n(x)}\right)
$$
donde $R_n(x)$ es la distancia euclídea entre $x$ y el $k$-ésimo vecino más cercano de $x$ entre los $X_i$, con $k=k(n)$  enteros tales que $k\to\infty$ y $k/n\to 0$, y $K(\cdot)$ es una función núcleo tal que 
$$
\int K(u)du=1
$$
\end{frame}


\begin{frame}
\frametitle{Medidas de bondad de ajuste}
Consideremos las siguientes medidas sobre nuestro estimador
$$
Bias(\hat{f}(x)) = E[\hat{f}(x)]-f(x) \qquad Var(\hat{f}(x)) = E\left[\left(E[\hat{f}(x)]-\hat{f}(x)\right)\right]
$$
$$
MSE(\hat{f}(x)) = E\left[(\hat{f}(x)-f(x))^2\right] = Bias(\hat{f}(x))^2 + Var(\hat{f}(x))
$$
$$
MISE = \int MSE(\hat{f}(x)) 
$$
Consideramos además el AMISE como el \textit{asymptotic} MISE.
\end{frame}


\begin{frame}
\begin{teorema}[Mack and Rossenblatt] Sea $f$ fdd acotada. Supongamos que la función núcleo verifica
$$
\int |x|^2 |K(x)|dx < \infty \qquad \int |x|K(x) dx = 0
$$
Además, sea $x$ un punto donde $f(x)>0$ y supongamos que $f$ es $\mathcal{C}^2$ en un entorno de $x$. Entonces
$$
Bias(\hat{f}(x)) = \frac{1}{2^3}\left(\frac{k}{n}\right)^2\mu_2(K)\frac{f''(x)}{f^2(x)}+o\left(\left(\frac{k}{n}\right)^2+\frac{1}{k}\right)
$$
$$
Var(\hat{f}(x)) = \frac{2f(x)^2}{k}\int K(x)^2 dx+o\left(\frac{1}{k}\right)
$$
\end{teorema}
\end{frame}



\begin{frame}
\begin{coro}
En las condiciones del teorema anterior, tenemos que
$$
MSE(\hat{f}(x))\to 0
$$
y, por tanto,
$$
\hat{f}(x)\overset{P}{\longrightarrow}f(x)
$$
\end{coro}
\end{frame}
\begin{frame}
\frametitle{Bibliografía}
\begin{enumerate}
\item Härdle, W. (1990). Applied Nonparametric Regression (Econometric Society Monographs). Cambridge: Cambridge University Press. doi:10.1017/CCOL0521382483
\item Orava, J. (n.d. ). K-nearest neighbour kernel density estimation, the choice of optimal k, Tatra Mountains Mathematical Publications, 50(1), 39-50. doi: https://doi.org/10.2478/v10127-011-0035-z
\item Lian, Heng. (2011). Convergence of functional k-nearest neighbor regression estimate with functional responses. Electronic Journal of Statistics. 5. 10.1214/11-EJS595. 
\end{enumerate}

\end{frame}

\end{document}
