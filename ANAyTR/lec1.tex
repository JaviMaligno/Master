\documentclass[ANAyTR.tex]{subfiles}

\begin{document}


%\hyphenation{equi-va-len-cia}\hyphenation{pro-pie-dad}\hyphenation{res-pec-ti-va-men-te}\hyphenation{sub-es-pa-cio}

\chapter{Teoría de Representaciones}
\section{Álgebras (asociativaas con unidad sobre un cuerpo $k$)}
Algunas álgebras habituales que usaremos seran las álgebras de polinomios, el álgebra de endomorfismos de un espacio vectorial, el álgebra de matrices cuadradas de orden $n$ (esta es la misma que la anterior para dimensión finita) y álgebras asociativas libres (polinomios no conmutativos). 

\begin{defi}
Una \emph{representación} de un álgebra $A$ es un $A$-módulo a la izquierda. También se puede interpretar como un espacio vectorial $V$ sobre $k$ (el mismo cuerpo que el álgebra) con una operación extra $A\times V\to V$ que extiende a la operación $k\times V\to V$ ``razonablemente''. 
\end{defi}

\begin{ej}[Ejemplo fundamental]
Sea $V$ un espacio vectorial de dimensión finita sobre $\C$ ($V=\C^n$). Sea $f\in\mathrm{End}(V)$ un endomorfismo con matriz asociada $A$ respecto a la base canónica. Si tenemos la expresión $(f^2-f-id)(v)$, podemos representarla por $(A^2-A-I)X$, donde $X$ es el vector de coordenadas de $v$. Vemos que ambas expresiones son el mismo polinomio evaluado en dos tipos de objetos distintos. Así que si consideramos un polinomio $P$, tenemos un producto $P\cdot v=P(f)(v)$. Esto refleja la idea de extender ``razonablemente'' el producto por escalar. Decimos entonces que $f$ tiene asociada una \emph{representación} del álgebra de polinomios $\C[x]$ (en general $k[x]$), que es una acción del álgebra de polinomios sobre el espacio vectorial.
\end{ej}


En vistas del ejemplo anterior, podemos precisar que la aplicación $\varphi:A\times V\to V$ debe cumplir las siguientes propiedades:
\begin{enumerate}
\item $\varphi(a,v+w)=\varphi(a,v)+\varphi(a,w)$
\item $\varphi(a,\lambda v)=\lambda\varphi(a,v)$
\item $\varphi(a+b,v)=\varphi(a,v)+\varphi(b,v)$
\item $\varphi(\lambda a,v)=\lambda\varphi(a,v)$
\item $\varphi(1,v)=v$
\item $\varphi(ab,v)=\varphi(a,\varphi(b,v))$. 
\end{enumerate}

Definimos $\rho:A\to \mathrm{End}(V)$, $\rho(a): v\mapsto\varphi(a,v)$. Las reglas anteriores nos dicen que $\rho$ es lineal, es decir, $\rho(a)(v+w)=\rho(a)(v)+\rho(a)(w)$, $\rho(a)(\lambda v)=\lambda\rho(a)(v)$. Además $\rho(a+b)=\rho(a)+\rho(b)$, $\rho(\lambda a)=\lambda\rho(a)$, $\rho(1)=id_V$ y $\rho(ab)=\rho(a)\circ\rho(b)$. Estas últimas propiedades se resumen en que $\rho$ es un morfismo de álgebras. Esto nos da la siguiente definición más concisa:

\begin{defi}
Una representación de un álgebra $A$ sobre $k$ es un morfismo de álgebras $A\to\mathrm{End}(V)$ donde $V$ es un espacio vectorial sobre $k$. Decimos también que $V$ es una representación de $A$.
\end{defi}

En el caso del ejemplo anterior, cada $f\in\End(V)$ da a $V$ una estructura de representación de $k[x]$ dada por $k[x]\to\End(V)$ donde $P\mapsto P(f)$. 

\begin{defi}
Una \emph{subrepresentación} de una representación $V$ de $A$ es un subespacio vectorial $L$ de $V$ estable bajo multiplicación por los elementos de $A$, esto es $\forall a\in A$, $\forall v\in L$, $a\cdot v\in L$ (identificamos la representación con $\rho:A\to\End(V)$ y $\rho(a)(v)$ con $a\cdot v$, además sería necesario definir $\rho'(a)=\rho(a)|_L$ como subrepresentación). 
\end{defi}

\begin{defi}
Si $V_1$ y $V_2$ son dos representaciones de $A$, un morfismo de representaciones de $V_1$ en $V_2$ es una alicación lineal $f:V_1\to V_2$ tal que $\forall a\in A$, $\forall v\in V_1$, $f(av)=af(v)$ (interpretar como $f\circ \rho_1(a)(v)=\rho_2(a)\circ f(v)$, donde $\rho_i$ son las representaciones correspondientes). Más guay si lo pensamos com ouna transformación natural: para todo $a\in A$ este diagrama conmuta
\[
\begin{tikzcd}
V_1\arrow[r, "f"]\arrow[d, "\rho_1(a)"] & V_2\arrow[d, "\rho_2(a)"]\\
V_1 \arrow[r, "f"] & V_2
\end{tikzcd}
\]
También se les llama a estos morfismos \emph{intertwining operators}. Un \emph{isomorfismo} de representaciones es un morfismo con two-sided inverse o equivalentemente, un morfismo biyectivo. 
\end{defi}

\begin{ej}
Sea $V$ un espacio vectorial de dimensión 3 sobre $\C$ y sea $f$ un endomorfismo de $V$ que admite, con respecto a cierta base $\{u_1,u_2,u_3\}$, la matriz
\[
\begin{pmatrix}
2 & 1 & 0\\
0 & 2 & 0\\
0 & 0 & 3 
\end{pmatrix}
\]
A $f$ se le asocia una representación de $\C[x]$, $\C[x]\to\End(V)$ mediante $P\mapsto P(f)$. Tenemos que $f(u_1)=2u_1$. Así que por definición $P\cdot u_1=P(f)(u_1)$. En particilar, $x\cdot u_1=f(u_1)=2u_1$. Con $x^2$ sería $f^2(u_1)=f(f(u_1))=2^2u_1$. En general, es fácil observar que $P\cdot u_1=P(2)\cdot u_1$. Tenemos que la recta $\C u_1$ es una subrepresentación de la representación considerada. Lo mismo se puede hacer con $u_3$, por lo que $\C u_3$ es otra subrepresentación. De la matriz obtenemos también que $\C u_1\oplus \C u_2$ es una subrepresentación. Se deja como ejercicio comprobar que cualquier otra recta no es estable bajo el producto, por lo que hemos obtenido todas las ubrepresentaciones. 

Si tuviéramos una matriz diagonal, las subrepresentaciones serían las de dimensión 1 y las generadas por sumas directas de ellas. Descompondríamos así la representación en subrepresentaciones \emph{irreducibles}, que las definimos a continuación.
\end{ej}

\begin{defi}
Una representación es \emph{irreducible} cuando no es $\{0\}$ y no contiene ninguan subrepresentación propia. Una representación es \emph{indecomponible} cuando no se puede obtener como suma directa de subrepresentaciones no triviales. 
\end{defi}

El ejemplo anterior muestra que las dos nociones definidas no son equivalentes. Se deja como ejercicio comprobar que para dimensión finita, se puede proceder a descomponer la representación en suma directa de subrepresentaciones y reiterar hasta que tenemos la representación descompuesta como suma directa de indecomponibles. 

Para cualquier endomorismo $f$ de un espacio vectorial $V$ de dimensión finita sobre $\C$ ha una base con respecto a la cual la matriz de $f$ tiene la forma normal de Jordan (diagonal por bloques, donde los bloques tienen $\lambda$ en la diagonal y 1 en la superdiagonal). Esto se interpreta en términos de representaciones como que cualquier representación de $\C[x]$ indecomponible de dimensión finita es del tipo $P\mapsto P(A)$, donde $A$ es un bloque de Jordan. Estas representaciones están indexadas por pares $(\lambda,r)$, donde $\lambda$ es el coeficiente de la diagonal y $r$ es el tamaño de la matriz. 


\begin{defi}
Dado un grupo finito $G$, el \emph{álgebra} $k[G]$ del grupo es el espacio vectorial con base los elementos de $G$ con el producto bilineal que existiende la multiplicación de $G$. No hay problema en definirlo para un grupo finito, pero no vamos a ver representaciones de grupos infinitos. 
\end{defi}

\begin{ej}
Sea $S_2=\{Id,\tau\}$ el grupo de dos elementos. Entonces $k[S_2]$ son las expresiones de la forma $(aId+b\tau)(cId+d\tau)$. 
\end{ej}

\begin{lemma}[de Schur]
Sea $A$ un álgebra, $V_1,V_2$ dos representaciones de $A$ y $f:V_1\to V_2$ un morfismo de representaciones. Entonces:
\begin{enumerate}
\item Si $V_1$ es irreducible, entonces $f$ es 0 o inyectiva.
\item Si $V_2$ es irreducible, entonces $f$ es 0 o sobreyectiva.
\item Si $V_1$ y $V_2$ son irreducibles, entonces $f$ es 0 o isomorfismo. 
\end{enumerate}
\end{lemma}
\begin{proof}\
\begin{enumerate}
\item $\ker f$ es una subrepresentación de $V_1$, es decir, $\forall a\in A$ y $\forall v\in\ker f$ se tiene que $av\in\ker f$. Esto es fácil de ver, si $v\in\ker f$, $f(v)=0$, pero entonces $af(v)=f(av)=0$, luego $av\in\ker f$. Como $V_1$ es irreducible, entonces solo hay dos posibilidades, $\ker f=0$ ($f$ inyectiva) o $\ker f=V_1$ ($f=0$). 
\item Para la imagen igual porque también es una subrepresentación de $V_2$.
\item Unir resultados anteriores. 
\end{enumerate}
\end{proof}

\begin{coro}[lema de Schur para $k$ algebraicamente cerrado]
Sea $A$ un álgeba y $V$ una representacion irreducible de $A$ de dimensión finita. Sea $f:V\to V$ un morfismo de representaciones. Como $k$ es algebraicamente cerrado, $f$ tiene un autovalor $\lambda\in k$. Se tiene que $f-\lambda Id$ es un morfismo de representaciones. Este morfismo no es inyectivo (si $v$ es un autovector, ($f-\lambda Id)(v)=0$), por lo que no puede ser un isomorfismo, así que es 0, es decir, $f=\lambda Id$. 
\end{coro}
\begin{proof}
$a(f-\lambda Id)(v)=af(v)-a\lambda v=f(av)-\lambda av=(f-\lambda Id)(av)$.
\end{proof}


\section{Ideales}

Definición

Weyl algebra (1.7)

\begin{prop}
Los monomios $\{x^iy^j\}$ son una base para el álgebra de Weyl $W$.
\end{prop}
\begin{dem}[Independencia lienal]
Consideramos la representación $W\to\End(\R[t])$ expresada como la derivada. Obsérvese que la derivada de $t\cdot f$ es $f+tf'$. En términos del operador $\partial=\frac{d}{dt}$ y la función $x: f\mapsto tf$ tenemos que $\partial x(f)=f+x\partial (f)$. Esta identidad es equvialente a $\partial x=1+x\partial$ (de aquí la relación del cociente). Esta representación está bien definida porque es la aplicación inducida en el cociente de $k\gene{x,y}\to\End(\R[t])$ dada por $x\mapsto t$ (multiplicación por $t$), $y\mapsto\partial$, ya que $yx-xy-1\mapsto \partial x-x\partial -1=0$. 

Sea $L=\sum a_{ij}x^iy^j\in W$ y suponemos que $L=0$. Debemos demostrar que todos los $a_{ij}=0$. $L$ se puede escribir también como $\sum P_j(x)y^j$ con $P_j(x)=\sum a_{ij}x^j$. Como $L=0$, también su imagen por la representación lo es, es decir, $\sum P_j(x)\partial^j=0$ como elemento de $\End(\R[t])$ (entiendiendo $x$ como multiplicación por $t$). Consideramos $(\sum P_j(x)\partial^j)1=P_0(x)1=P_0(t)$ (todas las derivaciones se anulan en la constante). Consideramos también 
\[
(\sum P_j(x)\partial^j)t=(P_0(x)+P_1(x)\partial)(t)=tP_0(t)+P_1(t)
\]
Podemos continuar aumentando el grado, y por hipótesis todos los polinomios se anulan. Por inducción se prueba que $P_i(t)=0$ para todo $i$. Esta prueba da problemas para característica positiva. En el libro se hace una modificación para salvar este problema.

\end{dem}


\section{Quivers}

Puñalada a quien le llame carcaj

Lo resumo en que un quiver es una categoría pequeña en la que no dibujamos las identidades ni las composiciones, y una representación de un Quiver es un functor entre este categoría y la categoría de espacios vectoriales sobre un cierto cuerpo. Por tanto, un morfismo de representaciones no es más que una transformación natural. 

Debo pensar cómo categorizar la definición de subrepresentación en términos de los morfismos de inclusión. 

\section{Álgebras de Lie}
La identidad de Jacobi sale de que el corchete es una derivaciónnnnnn

Recordemos que la antisimetría salía de que $[a,a]=0$.

\begin{teorema}
Todo álgebra de Lie de dimensión fintia es isomorfo a un subálgebra de un álgebra de Lie de matrices $M_n(k)$ con el corchete $[A,B]=AB-BA$. 
\end{teorema}


Las representaciones $g\to\End(V)$ están en biyección con las familias de elementos de $\End(V)$ que cumplen la relación $[x_i,x_j]=\sum c_{ij}^kx_k$, que a su vez están en biyección con las representaciones de $U(g)$. 


\section{Productos tensoriales}

\section{Álgebra tensorial}
El álgebra libre se consigue porque un elemento es de la forma $a+\sum a_i x_i+\sum a_{ij}x_i\otimes x_j+\sum a_{ijk}x_i\otimes x_j\otimes x_k+\cdots$ (suma finita) y entonces los monomios estos son los del álgebra libre asociativa. 

El álgebra exterior se puede ver como cociente por $v\otimes v$ pero también como cociente por $u\otimes v+v\otimes u$. 

\section{Subrepresentaciones en álgebras semisimples}

No veremos demostraciones porque son aburridas (sic)

Si $V$ es una representación de $A=k[x]$ dimensión finita, es semisimple si y solo si la matriz de la representacion $F=\rho(x)$ es diagonalizable, ya que si no lo es, mirando en la forma de Jordan, hay subrepresentaciones irreducibles dentro de cada factor. 

Proposición 2.2

Teorema de densidad

\section{2.3}

Hacer el ejercicio

Recordar que la sobreyectividad implica una relación entre las dimensiones.


\section{2.5}
Por ejemplo, si consieramos $\C[x]/\gene{x^2}$, como $x$ es nilpotente, debe tener imagen 0 en cualquier representación irreducible, ya que $\rho(x)(v)=\lambda x$ por Schur, pero $\rho(x)^2=\rho(x)^2=0$, mientras que $\rho(x)^2(v)=\lambda^2 v$, por lo que $\lambda=0$. 

Que actúe coo 0 en irreducibles, implica que actúa como 0 en semisimples, que son suma de irrep.

Si $Rad(A)=0$, entonces $A$ es isomorfa a una suma de álgebras de matrices, además todas las representaciones de dimensión finita son semisimples, en particular, la representación regular de $A$ es semisimple (en este caso se dice que $A$ es semisimple) TODO ESTO SON CARACTERIZACIONES.

 En caso contrario, $A$ no es de esa forma y la representación de $A$ no es semisimple, ya que $\rho(a)(1)=a\cdot 1=a\neq 0$. En consecuencia, no todas las representaciones de dimensión finita de $A$ son semisimples.
 
 \section{2.6}
 
 Si $V$ es semisimple de $A$ y $V\cong\oplus n_i V_i$, entonces $\chi_V=\sum n_i\chi_{V_i}$, donde los $n_i$ están vistos como elementos del cuerpo (es decir, en característica positiva serían sus clases de equivalencia). 
 
 
 Como $Tr(AB-BA)=0$ por ser $Tr(AB)=Tr(BA)$ y linealidad, $Tr([A,B])=0$. 
 
 Si $k$ algebraicamente cerrado, $A$ de dimensión finita es semisimple con irrep $V_1,\dots, V_r$, los caracteres asociados a los $V_i$ son linealmente independientes en $A^*$. Como consecuencia, son una base del dual del derivado $(A')^*$
 
 (mirar sección 2.4)
 
 Si $\rho:A\to\End(V)$ es una representación y $V_1$ es una subrepresentación, sobre $V/V_1$ se define una representación como $\rho(a)(v\mod V_1)=\rho(a)(v)\mod V_1$
 
 Teorema de Jordan-Hölder
 
 \section{Representaciones de grupos finitos}
 Todo algebraicamente cerrado 
 
 Operador de Reynolds ($P$ página 33), es una proyección lineal con imagen el subespacio de los elementos invariantes. Una proyección lineal es un endomorfismo idempotente. 
 
 La idea de la prueba de 3.1 es modificar la proyección del operador de Reynolds para tener otra proyección, que será un morfismo de representaciones y que tendrá la misma imagen. Es como si en lugar de tomar el complementario, tomas otro un poco más torcido para que sea subrepresentación.
 
 Si la característica de $k$ divide a $|G|$, entonces la suma de los elementos de $G$ es un elemento central y nilpotente (elevas al cuadrado y te sale la suma multiplicada por el cardinal, que es 0 en $k$). El producto por este elemento, usando el lema de Schur de que actúa como multiplicación por escalar, es 0 (porque el escalar al cuadrado tiene que ser 0), es decir, que pertenece al radical y por tanto $k[G]$ no es semisimple. 
 
 Lo de que $|G|=\sum \dim(V_i)^2$ es porque $|G|=\dim k[G]=\sum\dim(V_i)^2$. Se tiene $k[G]\cong \oplus\End(V_i)$ como álgebras y $k[G]\cong\oplus\dim(V_i)V_i$ como representaciones de $G$. 
 
 Recordamos que los caracteres de las irreps forman una base del dual del abelianizado de $k[G]$. 
 
 \begin{ej}
 
 Representaciones de $S_3$. La representación natural está dada por la permutación de una base de $k^3$. Hay 2 formas de descomponer $6=|S_3|$ como suma de cuadrados. Una de ellas implicaría que todas las representaciones irreducibles son de dimensión 1, pero entonces $S_3$ sería conmutativo porque se correspondería con matrices diagonales. Por tanto, la descomposición es $2^2+1^2+1^2$, por lo que hay una representación de dimensión 2, y dos de dimensión 1, una de ellas triviales. La trivial se puede ver como la recta generada por $e_1+e_2+e_3$ en $k^3$. Una subrepresentación es el plano $x_1+x_2+x_3=0$, porque permutar las coordenadas no altera la suma. Es irreducible porque podemos tomar los vectores $e_1-e_2, e_2-e_3,e_3-e_1$ y encontramos una permutación que lleva cada uno en el otro, concretamente la que hace $e_i\mapsto e_{i+1}$, luego no tiene rectas fijas (esta representación se suele llamar estándar). La que falta de dimensión 1 se obtiene mediante la representación alternada, correspondiente al signo de las permutaciones (la acción es multiplicar por el signo, no importa qué recta cojamos si la hacemos isomorfa a $k$).
 \end{ej}
 
La tabla de caracteres se hace calculando la traza de la matriz de cada elemento del grupo en cada una de las representaciones irreducibles. La traza de la identidad te da la dimensión de la representación. Elementos de una misma clase de conjugación dan las mismas trazas así que basta con calcular una matriz en que el elemento $(i,j)$ sea la traza de la matriz asociada a la representación $j$ de un elemento de la clase de conjugación $i$. (Teorema) Hay tantas irreps como clases de conjugación, por lo que la matriz sale cuadrada. Recordemos que en un grupo conmutativo hay una clase por cada elemento. 

\section{3.4}
Definición de producto tensorial de representaciones

Representación dual (de $V^*$) se define como $g(L)(v)=L(g^{-1}v)$. El motivo es, además de que hay que conseguir que salga un morfismo de álgebras, que $g(L)(gv)=L(v)$. El caracter de la representación dual es el caracter no dual del inverso. 

\section{3.5}

La traza del producto tensorial es el producto de las trazas. Se puede hacer a lo bruto cogiendo bases, mirando las matrices resprectivas y calculando la imagen de $f(v_i)\otimes f(v_j)$, y queda que los coeficientes son los productos de los coeficientes (producto de Kronecker de matrices) \url{https://en.wikipedia.org/wiki/Kronecker_product}

El caracter de la suma directa es la suma de los caracteres (la matriz es coger las de cada base y ponerlas en bloques en diagonal) 


\section{3.8}
Tabla de caracteres de $S_4$. La representación natural (como matrices de permutación) se descompone como la trivial y la estandar (permutar coordenadas de un hiperplano), esta suma dada por el hiperplano y la recta normal. Así que se calcula el caracter de la estandar a partir de estas dos. La traza de $(12)(34)$ es 0 porque los elementos de la diagonal se corresponden con puntos fijos de la permutación. En general se pueden calcular las de todas las clases así. 

Para ver que la estándar es irreducible se usa que si $V=\oplus n_i V_i$ entonces el producto escalar de su caracter consigo mismo es $\sum n_i^2$. Es necesario y suficiente que esta suma de 1. Como es de dimensión 3 y en el caso complejo de grupos finitos son semisimples (se descomponen), podemos ver que no contiene como subrepresentación ninguna de dimensión 1 (ya que o se descompone como tres de 1 o como una de 2 y una de 1).

Para sacar la de dimensión 3 que falta hacemos producto tensorial de estándar y alternada. Como los caractares se obtienen con el producto, se calculan fácilmente. Y ver si es irreducible se hace igual que antes (o con la descomposición del producto tensorial que se saca con los caracteres).

Para la última (de dimensión 2) se saca por ortonormalidad o ortogonalidad tomando suma en representaciones. Para saber qué forma tiene, se puede usar la proyección del ejercicio de nilpotentes sobre la represenación regular.  


Una forma de ver la sobreyección $S_4\to S_3$ es la acción por conjugación, viendo $S_3$ como permutaciones de los 3 elementos de la clase de conjugación de $(12)(34)$. Otra es esta \url{https://math.stackexchange.com/questions/2191483/constructing-a-homomorphism-from-s4-to-s3-that-satisfies-specific-conditions}. La representación de dimensión 2 se obtiene a partir de la representación estándar de $S_3$. 


\section{4.6}

Dados dos grupos $G$, $H$, las irresp de $G\times H$ son de la forma $V\otimes W$ para $V$ irrep de $G$ y $W$ irrep de $H$.

Es fácil comprobar que que las clases de conjugación de $G\times H$ son los productos de clases de conjugación.  Para comprobar que los productos tensoriales de irreps, se calculan los productos escalares $\gene{\chi_{V_i\otimes W_j},\chi_{V_i\otimes W_j}}$ usando que $\chi_{V_i\otimes W_j}(g,h)=\chi_{V_i}(g)\chi_{W_j}(h)$, es decir, sale el producto de dos productos escalares que son 1. 

Como los caracteres forman una base del espacio de funciones de clase, cualquier función de clase se escribe de forma única como combinación lineal de ellos. Esto quiere decir que si el caracter de una representación es combinación lineal de otros caracteres, entonces esa representación se descompone como combinación lineal de las representaciones asociadas. De hecho, podemos resolver cualquier descomposición resolviendo un sistema lineal. 


Si $V$ es una representación de $G$, entonces $S^nV$ y $\Lambda^nV$ también lo son (la representación es aplicar el elemento a todos los factores).


Cálculo de $\chi_{S^2(V)}$. Llamamos $S^2f$ a la aplicación $uv\in S^2V\mapsto f(u)f(v)\in S^2V$. Pongamos que $\dim V=2$ y $f$ es diagonalizable con base $\{e_1,e_1\}$ con respecto a la cual su matriz es diagonal. Si los autovalores son $\lambda_1,\lambda_1$, entonces $Tr(f)=\lambda_1+\lambda_2$. $S^2V$ admite como base $e_1^2, e_1e_2, e_2^2$. Calculamos $S^2f$ de estos vectores para obtener su matriz con respecto a esa base, que sale diagonal y la traza es $\lambda_1^2+\lambda_1\lambda_2+\lambda_2^2$. Vamos a creernos que esto funciona cuando $f$ no es diagonalizable. Tendríamos además $\chi_V(g^2)=\lambda_1^2+\lambda_2^2$. Por otra parte, $\chi_V(g)^2=\lambda_1^2+\lambda_2^2+2\lambda_1\lambda_2$, y con todo esto podemos ya expresarlo en términos conocidos. En concreto, $\chi_{S^2V}=\frac{\chi_V(g)^2+\chi_V(g^2)}{2}$. Esta identidad de trazas vale para todas las matrices diagonales, que son un subespacio denso del espacio de todas las matrices, luego vale para todas por continuidad. 

Si no queremos usar topología, lo que sí sabemos es que las matrices serán triangularizables (pueden ponerse en bloque de Jordan) y la diagonal siguen siendo los autovalores (en dimensión 2 hay un coeficiente que no conocemos pero da igual). Haciendo el mismo razonamiento se obtiene el mismo resultado.

En general, $Tr(S^2f)=\sum\lambda_i^2+\sum_{i<j}\lambda_i\lambda_j$, $Tr(f)=\sum\lambda_i$, $Tr(f^2)=\sum\lambda_i^2$, $Tr(f)^2=\sum_{i<j}\lambda_i^2+2\sum\lambda_i\lambda_j$. Así que el resultado $Tr(S^2f)=\frac{Tr(f)^2+Tr(f^2)}{2}$ es válido para cualquier dimensión. 

Si $V$ es una representación compleja de $G$ a partir de cualquier producto escalar sobre $V$ construimos un producto escalar invariante por $G$
\[
\gene{u,v}=\frac{1}{|G|}\sum_{g\in G}\gene{g(u),g(v)}
\]
Que sea invariante por $G$ quiere decir que tos los elementos de $G$ actúan como isometrías, es decir, $\gene{h(u),h(v)}=\gene{u,v}$ para todo $h\in G$ y para todo $u,v\in V$ (los $\rho(h)$ son endomorfismos unitarios). 

Si $V$ es irreducible, entonces hay un único producto escalar invariante sobre $G$ (salvo multiplicación por escalar). Esto aunque no lo parezca es una consecuencia del lema de Schur. Es claro que existe uno porque acabamos de construirlo. Un producto escalar induce un isomorfismo $V\to V^*$ dado por $v\mapsto \gene{v,}$. En el caso complejo no es un isomorfismo porque en una de las coordenadas los escalares salen conjugados. Para ello definimos el espacio conjugado $\overline{V}$ como $V$ pero el producto por escalar es $\lambda\cdot v=\overline{\lambda}v$. Esto entonces hace que la aplicación anterior sea isomorfismo como aplicación $\overline{V}\to V^*$ (y en general convierte semilineales en lineales).

Si $\varphi\in\Hom_{Vect}(\overline{V},V^*)$, la propiedad de ser invariante bajo $G$ se puede expresar como $\gene{h(u),w}=\gene{u,h^{-1}(w)}$. Entonces $\varphi(h(u))(w)=\varphi(u)\circ h^{-1}(w)$ para $w=h(v)$.  La acción sobre el dual era precisamente multiplicar por el inverso, así que $\varphi(\rho_V(h)(u))=\rho_{V^*}(h)(\varphi(u))$. 

Este cálculo demuestra que $\varphi$ es de hecho un morfismo de representaciones, y por el lema de Schur $\overline{V}\cong V^*$, y además es único salvo producto por escalar, luego $\dim Hom_G(\overline{V},V^*)=1$. 

\section{Restricción e inducción de representaciones (4.8)}
Sea $G$ un grupo finito y $H\leq G$ un subgrupo. Si $V$ es una representación de $G$, también lo es de $H$ por composición $H\hookrightarrow G\to GL(V)$. Esta representación se llama restricción de $V$. Si $V$ es irreducible para $G$, no tiene por qué serlo para $H$. Mirando en la tabla de caracteres, por ejemplo de $S_3$ y tomando como subgrupo $S_2=\gene{(12)}$, evidentemente sobra una fila (porque quitamos la otra columna), la última es suma de las dos primeras. 

Las aplicacioens $G\to W$ se pueden pensar como familias de vectores indexadas por elementos del grupo. 


\begin{teorema}[4.33, Frobenius Reciprocity]
$\Hom_G(V,Ind^G_HW)\cong \Hom_H(Res^G_H,W)$ (isomorfismo natural de espacios vectoriales)
\end{teorema}
$\dim \Hom_G(M,N)=\gene{\chi_M,\chi_N}$. 

Como consecuencia, $\gene{\chi_{Res^G_H V},\chi_W}=\gene{\chi_V,\chi_{Ind^G_H}}$. 

En algunos libros definen la inducida como $k[G]\otimes_{kH}W$, que para grupos finitos es más conveniente. El isomorfismo está dado por $f\mapsto \sum_{\sigma} x_\sigma\otimes f(x_\sigma^{-1})$, donde la suma es sobre los right-cosets.
 
 
 También se tiene la reciprocidad $\Hom_G(Ind^G_H,V)\cong\Hom_H(W,Res^G_H)$. Esto significa que cualquier morfismo de representaciones de $H$ $F:W\to V$ se descompone de forma única como $W\hookrightarrow Ind^G_H \to V$. A la aplicación de la derecha se la denota $\tilde{F}$ y es la única extensión de $F$ a $Ind^G_H$ que llega a $V$. 
\section{4.6}

\section{4.12}

\section{4.17}


\end{document}

