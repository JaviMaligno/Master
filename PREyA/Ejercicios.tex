
\documentclass[twoside]{article}
\usepackage{../estilo-ejercicios}
\newcommand{\x}{{\mathbf{x}}}
\newcommand{\y}{{\mathbf{y}}}
%--------------------------------------------------------
\begin{document}

\title{Procesos Estocásticos. Aplicaciones}
\author{Rafael González López}
\maketitle

\begin{ejercicio}{1}
Sea $\{X_t\}_{t\in T}$ un PE tal que $X_t$ es continua $\forall t$ y que verifica la propiedad de incrementos independientes. Probar que para conocer la densidad conjunta bidemensional es suficiente con conocer la densidad margial de $X_s$ y de $X_{t-s}$, para $s>t$


\end{ejercicio}

\begin{solucion}De la definición de incrementos independientes tenemos que para $t_1 = 0$, $t_2 = t$, $t_3=s$, tenemos que 
$X_t - X_0$ y $X_s - X_t$ son independientes, pero $X_0 =0$, luego $X_t$ es independiente de $X_s-X_t$. Además
$$
\begin{pmatrix}
X_t\\
X_s
\end{pmatrix} = \begin{pmatrix}
1 & 0\\
1 & 1
\end{pmatrix} \begin{pmatrix}
X_t\\
X_s-X_t
\end{pmatrix} 
$$
Tengamos en cuenta que, dado la independencia de las variables
$$
f_{X_t,X_s-X_t}(x,y) = f_{X_t}(x)f_{X_s-X_t}(y)
$$
Finalmente, aplicando el Teorema del cambio de variable
$$
f_{X_t,X_s}(x,y) = f_{X_t,X_s-X_t}(x,y-x)\abs{\begin{pmatrix}
1 & 0\\
-1 & 1
\end{pmatrix}} = f_{X_t}(x)f_{X_s-X_t}(y-x)
$$
\end{solucion}
\newpage

\begin{ejercicio}{2} Demostrar que si $\{X_t\}_{t\in T}$ es un PE markoviano con variables absolutamente continuas entonces
$$
f_{X_{t_1},\dotsc,X_{t_n}}(x_1,\dotsc,x_n) = f_{X_{t_1}}(x_1)\cdots f_{X_{t_n}\mid X_{t_{n-1}} = x_{n-1}}(x_n)
$$
\end{ejercicio}

\begin{solucion}
Vamos a demostrarlo por inducción sobre el número de índices $n$. Para $n=2$ tenemos por definición de probabilidad condicionada
$$
f_{X_{t_1},X_{t_2}}(x_1,x_2) = f_{X_{t_1}}(x_1)f_{X_{t_2}|X_{t_1}=x_1}(x_2)
$$
Supongamos que es cierto para cualquier conjunto de índices de tamaño $n-1$ y veámoslo para $n$. Análogamente al apartado anterior, sabemos que
$$
f_{X_{t_1},\dotsc,X_{t_n}}(x_1,\dotsc,x_n) = f_{X_1,\dotsc,X_{n-1}}(x_1,\dotsc,x_{n-1}) f_{X_{t_n}|X_{t_1},\dotsc, X_{t_{n-1}}}(x_n) 
$$
Usando la hipótesis de inducción sobre el primer elemento obtenemos
$$
f_{X_{t_1},\dotsc,X_{t_n}}(x_1,\dotsc,x_n) = f_{X_{t_1}}(x_1)\cdots f_{X_{t_{n-1}}\mid X_{t_{n-2}}}(x_{n-1}) f_{X_{t_n}|X_{t_1},\dotsc, X_{t_{n-1}}}(x_n) 
$$
Por tanto, solo falta demostrar que bajo las hipótesis del enunciado, usando la definición de proceso markoviano, tenemos que
$$
f_{X_{t_n}|X_{t_1},\dotsc, X_{t_{n-1}}}(x_n) = f_{X_{t_{n-1}}|X_{n-1}}(x_n)
$$
Pero esto es una consecuencia inmediata, pues
$$
P(X_{t_n} \leq x_n \mid X_{t_1},\dotsc,X_{t_{n-1}}) = P(X_{t_n} \leq x_n \mid X_{t_{n-1}})
$$
Es equivalente en el caso continuo a
$$
\int_{-\infty}^{x_n}\frac{f_{X_{t_1},\dotsc,X_{t_n}}(x_1,\dotsc,x_{n-1},s)}{f_{X_{t_1},\dotsc,X_{t_{n-1}}}(x_1,\dotsc,x_{n-1})}ds =  \int_{-\infty}^{x_n}\frac{f_{X_{t_{n-1}},X_{t_n}}(x_{n-1},s)}{f_{X_{t_{n-1}}}(x_{n-1})}ds 
$$
Basta derivar y aplicar la definición.
\end{solucion}

\newpage


\begin{ejercicio}{3}
Resolver la ecuación homogénea relativa al PE de la ruina del jugador. Además, calcular el numero medio de jugadas antes de la ruina cuando $p = q$.

\end{ejercicio}

\begin{solucion}
La ecuación que tenemos que resolver es
$$
w_k = pw_{k+1} + qw_{k-1} 
$$
para $b<k<a$, con $w_a =1$ y $w_b = 0$. Dado que $p+q=1$ podemos hacer la siguiente manipulación
$$
w_k = pw_k  + qw_k = pw_{k+1} + qw_{k-1}  \Rightarrow w_{k+1}-w_k = \frac{q}{p}(w_k - w_{k-1})	
$$
Consideremos la recurrencia $h_k = w_{k+1}-w_{k} = (q/p)h_{k-1}$. Es claro que $h_{b+1} = (q/p)w_{b+1}$, pues $w_b=0$. Por tanto, $h_{b+2} = (q/p)^2w_{b+1}$ y en general
$$
w_{k+1}-w_k = \left(\frac{q}{p}\right)^{k-b} w_{b+1}
$$
para $b\leq k<a$.
Además, es claro que
\begin{align*}
w_{k+1}-w_{b+1} &= \sum_{i=b+1}^k (w_{i+1}-w_i) \\
& = \sum_{i=b+1}^k \left(\frac{q}{p}\right)^k w_{b+1}
\end{align*}
De donde
$$
w_{k+1}  = \sum_{i=b}^k \left(\frac{q}{p}\right)^k w_{1}
= \begin{cases}
w_1 \frac{\left(\frac{q}{p}\right)^{b}-\left(\frac{q}{p}\right)^{k+1}}{1-\left(\frac{q}{p}\right)} & p \neq 1\\
w_1 (k+1-b) & p=q
\end{cases}
$$
Imponiendo que $w_a = 1$ 
$$
1 = w_a = \begin{cases}
w_1 \frac{\left(\frac{q}{p}\right)^{b}-\left(\frac{q}{p}\right)^{a}}{1-\left(\frac{q}{p}\right)} & p \neq 1\\
w_1(a-b) & p=q
\end{cases} \Rightarrow w_1 = \begin{cases}
 \frac{1-\left(\frac{q}{p}\right)}{\left(\frac{q}{p}\right)^{b}-\left(\frac{q}{p}\right)^{a}} & p \neq 1\\
(a-b)^{-1} & p=q
\end{cases}
$$
Obtenemos finalmente la recurrencia
$$
w_k = \begin{cases}
 \frac{\left(\frac{q}{p}\right)^{b}-\left(\frac{q}{p}\right)^k}{\left(\frac{q}{p}\right)^{b}-\left(\frac{q}{p}\right)^{a}} & p \neq q\\
\frac{k-b}{a-b} & p=q
\end{cases} \qquad b\leq k\leq a
$$
\newpage
Para la segunda parte del problema, consideremos $T_k$ la variable aleatoria del número de partidas antes de la ruina sabiendo que tenemos $k$ unidades. Queremos calcular $d_k = E[T_k]$. Naturalmente
$$d_k=E[T_k|\text{Ganar}]P[\text{Ganar}]+E[T_k|\text{Perder}]P[\text{Perder}]$$
Ahora bien, la distriubución de $T_k$ sujeto a que ganamos, debe ser $1+T_{k+1}$, así como si está sujeto a que perdemos, $1+T_{k-1}$, luego obtenemos la ecuación
$$
d_k = p(1+d_{k+1})+q(1+d_{k-1})=1+pd_{k+1}+qd_{k-1}
$$
con las condiciones de contorno $d_b = d_a = 0$. Si nos fijamos, la ecuación homogénea asociada es la que resolvimos antes, por lo que una solucion general de la ecuación homogénea viene dada (para $p=q$) por $C_1+C_2k$. Por otra parte, es trivial comprobar que $-k^2$ es una solución particular de la ecuación, luego la solución general viene dada por
$$
-k^2 + C_1+ C_2k 
$$
Imponiendo las condiciones de contorno tenemos
\begin{align*}
-a^2 + C_1 + C_2a &=0\\
-b^2+C_1 +C_2b &=0
\end{align*}
Si despejamos, obtenemos $C_1 = -ab$ y $C_2 = a+b$, por lo que la solución del problema es
$$
d_k = -k^2 -ab + (a+b)k = (k-a)(b-k)
$$
\end{solucion}


\newpage


\begin{ejercicio}{4}
Calcular la función de densidad y probabilidad de $T_k$ y estudiar sus propiedades estocásticas. 
\end{ejercicio}
\begin{solucion}
Dado que los sucesos $X_i$ son i.i.d. a una ley $Be(p)$, tenemos que $N_n \sim Bi(n,p)$. Por las equivalencias vistas en teoría podemos escribir
$$
F_k(n) = P[T_k \leq n ] = P[N_n \geq k] = 1 - P[N_n < k] = 1 - P[N_n \leq k -1] = 1- \sum_{i=0}^{k-1}\binom{n}{i}p^iq^{n-i}
$$
$$
P[T_k = n] = P[N_{n-1} =k-1]P[X_n = 1] = \binom{n-1}{k-1}p^kq^{n-k}
$$
\begin{itemize}
\item \textbf{Incrementos independientes}. Basta probarlo para el caso $n=2$. Sean $m_1 < m_2 < m_3$ entonces podemos escribir
$$
T_{m_2}- T_{m_1}  = (T_{m_2}- T_{m_2-1}) + … + (T_{m_{1}+1}- T_{m_1})
$$
y análogamente para $T_{m_3}- T_{m_2}$. Por la proposición anterior, las diferencias se pueden escribir como suma de variables independientes, luego las sumas han de ser independientes.

\item \textbf{Incrementos estacionarios}. Se deduce inmediatamente a partir de la proposición, pues podemos expandir tanto $T_{t+h}-T_{s+h}$ como $X_t - X_s$ en sumas de $t-s$ variables independientes e identicamente distribuidas.

\item \textbf{Independencia}. Claramente no lo son, pues $T_1$ y $T_2$ (p.e) no lo son, ya que $T_1$ y $T_2-T_1$ sí lo son, luego 

$$
Cov(T_1,T_2-T_1) = 0 = Cov(T_1,T_2) + Var(T_1) \Rightarrow Cov(T_1,T_2) = Var(T_1). 
$$

Como $T_1$ no es constante, su varianza no puede ser nula.
\item \textbf{Proceso markoviano}. Naturalmente, se deduce a partir de la propeidad de incrementos independientes. Sean $t_1,\dotsc,t_n$, vamos a utilizar los incrementos independientes que probaremos después
\begin{gather*}
P[T_n =  k_n \mid T_1=k_1,\dotsc,T_{n-1}=k_{n-1}] \\= P[T_n -T_{n-1} =  k_n - k_{n-1}\mid T_1=k_1,\dotsc,T{n-1}-T_{n-2}=k_{n-1}-k_{n-2}]\\
=P[T_n-T_{n-1}=k_n - k_{n-1}]
\end{gather*}
Ahora bien, volviendo a aplicar incrementos independientes en $t_{n-1},t_n$ tenemos que 
\begin{align*}
P[T_n-T_{n-1}=k_n - k_{n-1}] &= P[T_n -T_{n-1} =k_n - k_{n-1}\mid T_{n-1}  = k_{n-1}]\\
&= P[T_n =k_n \mid T_{n-1} =k_{n-1}]
\end{align*}
\item \textbf{Cadena de Markov}. No lo es, pues el soporte no es finito.
\item \textbf{Martingala}. Por la propiedad Markoviana tenemos
\begin{align*}
E[T_n \mid T_1,\dotsc,T_{n-1}] &= \sum_{m=n}^\infty m P[T_n =m \mid T_1,\dotsc,T_{n-1}]\\
&= \sum_{m=n}^\infty m P[T_n =m \mid  T_{n-1}]\\
&=E[T_n \mid T_{n-1}] \\
&= E[T_n - T_{n-1} + T_{n-1} \mid T_{n-1}] \\
&= E[T_n] - E[T_{n-1}] + T_{n-1}
\end{align*}
Usando el cálculo del último apartado
$$
E[T_n \mid T_1,\dotsc,T_{n-1}] = T_{n-1} + \frac{p}{k-k^2}
$$
Por lo que nunca puede ser martingala.
\item \textbf{Estacionario}. Claramente no puede serlo, pues en particular las $T_k$ deberían ser idénticamente distribuidas.
\item \textbf{Débilmente estacionario}. Si calculamos explícitamente la esperanza de $T_k$ observamos
$$
E[T_k] = \sum_{n=k}^\infty n \binom{n-1}{k-1}p^k(1-p)^{n-k} = \frac{p}{k}
$$
Como para distintos $\forall k,k'$ con $k\neq k'$ las esperanzas son distintas, no puede ser débilmente estacionario.
\end{itemize}
\end{solucion}

\newpage

\begin{ejercicio}{5} Probar que el estado $E_j$ es recurrente si y solo sí 
$$
\sum_{n = 1}^\infty p_{jj}^{(n)} = \infty
$$
Por tanto, será transitorio si y solo si la serie es convergente.
\end{ejercicio}

\begin{solucion}
Podemos calcular las fg de $p_{ij}^{(n)}$ y $f_{ij}^{(n)}$, las cuales denotamos $P_{ij}(s)$ y $F_{ij}(s)$ respectivamente. Tengamos en cuenta que $p_{ij}^{(0)}=1$ si $i=j$ y $0$ en caso contrario, y que $f_{ij}^{(0)} = 0$ $\forall i,j$. Podemos escribir si $i\neq j$:
\begin{align*}
P_{ij}(s) &= \sum_{n=0}^\infty s^n p_{ij}^{(n)}\\
&= \sum_{n=0}^\infty s^n \left(\sum_{r=0}^{n}f_{ij}^{(r)}p_{jj}^{(n-r)}\right)\\
&=  \left(\sum_{n=0}^\infty s^n f_{ij}^{(n)}\right)\left( \sum_{n=0}^\infty s^n p_{jj}^{(n)}\right)\\
&=  F_{ij}(s)P_{jj}(s)
\end{align*}
Para $i=j$ el razonamiento es ligeramente distinto,
\begin{align*}
P_{ii}(s) &= \sum_{n=0}^\infty s^n p_{ii}^{(n)}\\
&= 1+\sum_{n=0}^\infty s^n \left(\sum_{r=0}^{n}f_{ii}^{(r)}p_{ii}^{(n-r)}\right)\\
&= 1+ \left(\sum_{n=0}^\infty s^n f_{ij}^{(n)}\right)\left( \sum_{n=0}^\infty s^n p_{jj}^{(n)}\right)\\
&=  1 + F_{ij}(s)P_{jj}(s)
\end{align*}
Despejando, obtenemos
\begin{align*}
P_{ii}(s) &= \frac{1}{1-F_{ii}(s)} 
\end{align*}
Por tanto, $$P_{jj}(1)=\infty \Leftrightarrow  F_{jj}(1)=1 \Leftrightarrow f_j = 1 \Leftrightarrow \text{$E_j$ es recurrente}$$ .
\end{solucion}
\newpage


\begin{ejercicio}{7}Supongamos una cadea de Markov no homogénea con $3$ estados y matriz de transición en el instante $n$ dada por
$$
P_n=
\begin{pmatrix}
1/2 & 1/2 & 0\\
0 & 0 & 1\\
1/(n+1) & 0 & n/(n+1)
\end{pmatrix}
$$
Demostrar que el estado $E_1$ es recurrente no nulo.
\end{ejercicio}

\begin{solucion}
Tenemos que $p_{11}^{(n)}=1/2$, por lo que
$$
\lim_{s\to 1} P_{11}(s)= \sum_{n=0}^\infty 1/2 = \infty
$$
Por el Teorema de Caracterización, el estado $E_1$ es recurrente. Además, por el teorema de caracterización de estados nulos, tenemos que para que fuese nulo debería tenerse que $p_{11}^{(n)}\to0$, pero no es así. Por tanto, el proceso es no nulo.

Vamos a probarlo por otro camino. Veamos directamente que $f_{11}=1$. Tenemos que
\begin{align*}
f_{11}^{(1)} &= \frac{1}{2}& f_{11}^{(2)} &= 0\\
f_{11}^{(3)} &= \frac{1}{2} \frac{1}{3+1} & f_{11}^{(4)} &= \frac{1}{2} \frac{3}{3+1}\frac{1}{4+1} = \\
f_{11}^{(n)} &= \frac{1}{2}\prod_{i=3}^{n-1}\frac{i}{i+1}\frac{1}{n+1} & f_{11}^{(n)} &= \frac{3}{2}\frac{1}{n(n+1)} 
\end{align*}
Por tanto
$$
f_{11} = \frac{1}{2} + 0 + \frac{3}{2}\sum_{n=3}^\infty \frac{1}{n(n+1)} = \frac{1}{2} + \frac{3}{2}\frac{1}{3} = 1
$$
\end{solucion}
\newpage


\begin{ejercicio}{8}Supongamos una cadea de Markov no homogénea con $3$ estados y matriz de transición en el instante $n$ dada por
$$
P=
\begin{pmatrix}
0 & 1/2 & 1/4 & 1/4\\
1/2 & 1/2 & 0 & 0 \\
0 & 0& 1 &0\\
0 &0 & 1/2& 1/2
\end{pmatrix}
$$
Demostrar que el estado $E_1$ es transitorio.
\end{ejercicio}

\begin{solucion}
Vamos a verlo utilizando la caracterización de $f_1$. Tenemos que
\begin{align*}
f_1^{(1)} &= 0 & f_1^{(2)} &= 1/4\\
f_1^{(3)} &= 1/8 & f_1^{(4)} &= 1/16
\end{align*}
Es claro que las $f_1^{(n)}$, por las características de la matriz, deben permanecer en el segundo estado $n-1$ veces antes de volver a $1$, luego $\forall n\geq 2$ $f_1^{(n)}=1/2^n$. Por tanto
$$
f_1 = \sum_{n=1}^\infty f_1^{(n)} = \sum_{n=2}^\infty \frac{1}{2^n} = \frac{1}{2} < 1
$$
Por tanto, $E_1$ es transitorio, pues la suma debería ser $1$ para que fuese recurrente.
\end{solucion}
\newpage

\begin{ejercicio}{9}
Probar que una cadena de Markov $S$ es finita entonces todos los estados no pueden ser transitorios. De hecho, tampoco puede tener estados recurrentes nulos.
\end{ejercicio}
\begin{solucion}
Veamos que siempre existe un estado recurrente. Sea $i$ fijo y sea $S$ el conjunto de estados, finito por hipótesis. Para $n$ fijo, se tiene naturalmente que
$$
1 = \sum_{j \in S} p_{ij}^{(n)}
$$
Por tanto, utilizando que todas las cantidades son positivas, podemos escribir y reordenar
$$
\infty = \sum_{n =0}^{\infty} 1 = \sum_{n =0}^{\infty}\sum_{j \in S} p_{ij}^{(n)} = \sum_{j \in S} \sum_{n =0}^{\infty} p_{ij}^{(n)}	
$$
Dado que $S$ es finito, existe al menos un $j$ tal que
$$
 \sum_{n =0}^{\infty} p_{ij}^{(n)}	 = \infty
$$
Por el Teorema de Caracterizción de estados recurrentes, tenemos que ese $E_j$ es un estado recurrente.

Por el Teorema de descomposición, si existen estados recurrentes nulos, estarán en ciertos conjuntos de estados irreducibles y cerrados de estados no nulos. Tiene sentido, por tanto, considerar la matriz $P$ restringida únicamente a estos conjuntos de estados. Por el teorema de caracterización de estados nulos, tenemos que si un estado $i$ es no nulo entonces $p_{ij}^{(n)}\to 0$. Por tanto,
$$
P^n \to 0
$$
Pero siempre ha de verificarse que la suma por filas ha de ser $1$, lo cuál es imposible dado que las filas tienen un número finito de elementos al ser $S$ finito.
\end{solucion}
\newpage
\begin{ejercicio}{10}
El proceso de Poisson $\{N(t)\}_{t\geq 0}$ satisface
\begin{itemize}
\item Es un proceso de Markov
\item Tiene incrementos independientes
\item Tiene incrementos estacionarios
\item Para cualesquiera $s\leq t$ y naturales $i\leq j$ las probabilidades de transición son
$$
P[N(t+s)=j\mid N(s)=i] = e^{-\lambda t}\frac{(\lambda t)^{j-i}}{(j-i)!}
$$
\end{itemize}
\end{ejercicio}
\begin{solucion}
Veamos punto por punto las demostraciones
\begin{itemize}
\item Sean $t_1,\dotsc,t_n$, vamos a utilizar los incrementos independientes que probaremos después
\begin{gather*}
P[N(t_n) =  k_n \mid N(t_1)=k_1,\dotsc,N(t_{n-1})=k_{n-1}] \\= P[N(t_n) -N(t_{n-1}) =  k_n - k_{n-1}\mid N(t_1)=k_1,\dotsc,N(t_{n-1})-N(t_{n-2})=k_{n-1}-k_{n-2}]\\
=P[N(t_n)-N(t_{n-1})=k_n - k_{n-1}]
\end{gather*}
Ahora bien, volviendo a aplicar incrementos independientes en $t_{n-1},t_n$ tenemos que 
\begin{align*}
P[N(t_n)-N(t_{n-1})=k_n - k_{n-1}] &= P[N(t_n)-N(t_{n-1})=k_n - k_{n-1}\mid N(t_{n-1}) = k_{n-1}]\\
&= P[N(t_n)=k_n \mid N(t_{n-1})=k_{n-1}]
\end{align*}
\item Sean $t_1<t_2\leq t_3<t_4$ tenemos que por el postulado $1$.
\begin{align*}
P[N(t_4)-N(t_3)& = i\mid N(t_2)-N(t_1)=j]\\
& = P[N(t_3,t_4]=i \mid N(t_1,t_2]=j] \\
&= P[N(t_3,t_4]=i] \\
&= P[N(t_3)-N(t_2)]
\end{align*}
\item Es consecuencia inmediata del segundo postulado, pues en particular, si $t>s$ y sea $h>0$
$$N(t+h)-N(s+h)=N(s+h,t+h] \sim N(t-s) $$
Y análogamente para $N(t)-N(s)$. 
\item Usando la propiedad de incrementos independientes
\begin{align*}
P[N(t+s)=j\mid N(s)=i] &= \frac{P[N(t+s)=j,  N(s)=i]}{P[N(s)=i]}\\
&=\frac{P[N(t+s)-N(s)=j-i,  N(s)=i]}{P[N(s)=i]}\\
&=\frac{P[N(t+s)-N(s)=j-i]P[N(s)=i]}{P[N(s)=i]}\\
&=P[N(t)=j-1]\\
&=e^{-\lambda t}\frac{(\lambda t)^{j-i}}{(j-i)!}
\end{align*}
\end{itemize}
\end{solucion}
\newpage

\begin{ejercicio}{11}
Probar que Ppra un proceso de Poisson no homogéneo, la variable incremento $X_{t+s}-X_{s}$ tiene una distribución de Poisson de parámetro $$\Lambda(t+s)-\Lambda(s)$$	
\end{ejercicio}
\begin{solucion}
Sea $p_{n}(s,t) = P(X_{t}-X_s = n ) $ para $n\geq 0$ y $t\geq s$. Sea $h\geq 0$.  Por independencia para $ t \geq 0$ y $h$ decreciente a 0
\begin{align*}
p_{0}(s,s+t+h)&= P[X_{s+t+h}-X_s = 0]\\
& P[X_{s+t}-X_s = 0]P[X_{s+t+h}-X_{s+t}=0]\\
& = p_{0}(s,s+t)[1- \lambda(s+t)h +o(h)]\\
p_{0}(s,s+t+h)-p_0(s,s+t) &= p_{0}(s,s+t)[-\lambda(t)h + o(h)]
\end{align*}
Si dividimos por $h$ y hacemos $h\to 0$, se obtiene que $p_{0}'(s,s+t) = -\lambda(t+s)p_{0}(s,s+t)$, cuya solución es $p_{0}(s,s+t)=e^{-(\Lambda(s+t)-\Lambda(s))}$ con la condición inicial de que $p_0(s,s)=1$. Calculemos ahora $p_{n}(s,s+t)$ para $n \geq 1$. Por independencia, para $t \geq 0$
\begin{align*}
p_n(s,s+t+h)&=P[X_{s+t+h}-X_s=n]\\
&=\sum_{i=0}^n p_i(s,s+t)p_{n-i}(s+t,s+t+h)\\
&= p_n(s,s+t)p_0(s+t,s+t+h) + p_{n-1}(s,s+t)p_1(s+t,s+t+h)+o(h)\\
&= p_{n}(t) [1- \lambda(s+t)h +o(h)] + p_{n-1}(t)[\lambda(s+t)h+o(h)] + o(h)\\
p_n(s,s+t+h) - p_n(s,s+t) &= -p_n(s,s+t)\lambda(s+t)h+p_{n-1}(s,s+t)\lambda(t)h + o(h)\\
p_n'(s,s+t)&=\lambda(s+t)(-p_n(s,s+t)+p_{n-1}(s+t))
\end{align*}
Ahora bien, manipulando algebraicamente la ecuación anterior, podemos escribir
$$
\frac{d}{dt}\left(e^{\Lambda(s+t)}p_n(s,s+t)\right) = \lambda(s+t)e^{\Lambda(s+t)}p_{n-1}(s,s+t)
$$
A continuación vamos a probar el resultado por inducción. Para $n=0$ lo hemos visto antes. Nuestra hipótesis de inducción es que
$$
p_n(s+t)=e^{-(\Lambda(s+t)-\Lambda(s))}\frac{(\Lambda(s+t)-\Lambda(s))^n}{n!}
$$
Por tanto
\begin{align*}
\frac{d}{dt}\left(e^{\Lambda(s+t)}p_{n+1}(s,s+t)\right) &= \lambda(s+t)e^{\Lambda(s+t)}p_{n}(s,s+t)\\
&= \lambda(s+t)e^{\Lambda(s+t)}e^{-(\Lambda(s+t)-\Lambda(s))}\frac{(\Lambda(s+t)-\Lambda(s))^n}{n!}\\
\frac{d}{dt}\left(e^{\Lambda(s+t)-\Lambda(s)}p_{n+1}(s,s+t)\right) &= \lambda(s+t)\frac{(\Lambda(s+t)-\Lambda(s))^n}{n!}\\
e^{\Lambda(s+t)-\Lambda(s)}p_{n+1}(s,s+t) &= C + \frac{(\Lambda(s+t)-\Lambda(s))^{n+1}}{(n+1)!}
\end{align*}
Tomando $t=0$ es claro que $C=0$. Por tanto, despejando obtenemos
$$
p_{n+1}(s,s+t) = e^{-(\Lambda(s+t)-\Lambda(s))} \frac{(\Lambda(s+t)-\Lambda(s))^{n+1}}{(n+1)!}
$$
\end{solucion}
\newpage


\begin{ejercicio}{12}
Sea un proceso de Poisson no homogéneo $\{X_{t} \}$ de parámetro $\lambda(t)$. Probar que el proceso $\{X_{\Lambda^{-1}}\}_{ t \geq 0}$  es un proceso de Poisson homogéneo de parámetro $1$ si $\Lambda(t)$ es estrictamente creciente.
\end{ejercicio}
Vamos a probarlo usando la definición de proceso de Poisson. Denotemos $Y_t = X_{\Lambda^{-1}(t)}$.
\begin{itemize}
\item En primer lugar, sabemos que $\Lambda(0)=0$, por tanto $\inf\{u\geq 0 \mid \Lambda(u) \geq 0\} = 0$. En particular, 
$$
Y_0 = X_{\Lambda^{-1}(0)} = X_0 = 0
$$
Por ser $\{X_t\}$ proceso de Poisson.
\end{itemize} 

\begin{solucion}

\end{solucion}

\end{document}