\documentclass[PREyA.tex]{subfiles}
\begin{document}

\chapter{Cadenas de Markov}
\section{Introducción}

\begin{example} El departamento de estudios de mercado de una fábrica estima que el 20$ \% $ de la gente que compra un producto un mes, no lo compra al siguiente. Además, el 30$\%$ de quienes no lo compran un mes lo adquirirán al mes siguiente. En una población de 1000 individuos 100 compraron el primer mes. ¿Cuántos lo compraran al mes próximo? ¿y dentro de 2?


\begin{equation*}
 P(C_{2}) = P(C_{1}) \cdot P(C_{2}\mid C_{1} ) + P(\overline{C_{1}}) \cdot P(C_{2}\mid \overline{C_{1}} ) = 0.1 \cdot 0.8 + 0.9 \cdot 0.3 = 0.35
\end{equation*}

\begin{equation*}
 P(C_{3}) = P(C_{2}) \cdot P(C_{3}\mid C_{2} ) + P(\overline{C_{2}}) \cdot P(C_{3}\mid \overline{C_{2}} ) = 0.35 \cdot 0.8 + 0.65 \cdot 0.3 = 0.35
\end{equation*}

\end{example}
\begin{defi}
	Sea $S$ un conjunto discreto. Una \textit{cadena de Marcov} es una sucesión de v.a. $\left\lbrace X_{n}\right\rbrace _{n \in \mathbb{N}}$ que toma valores en S y cumple la propiedad
	
	\begin{equation*}
	P(X_{n+1}=x_{n+1} \mid  X_{1}=x_{1},\dotsc ,X_{n}=x_{n}) = P(X_{n+1}=x_{n+1}\mid X_{n}=x_{n})
	\end{equation*}
	para todo $x_{1},\dotsc ,x_{n+1}\in S$.
	
\end{defi}

\begin{nota}
El conjunto $S$ es el espacio de estados de la cadena de Marcov.	
\end{nota}
\begin{defi}
Diremos que la cadena visita el estado $i$ en el instante $n$ si $X_{n}=i$.
\end{defi}

\section{Probabilidades de transición}
\begin{defi}
Si las probabilidades de la ecuación anterior no dependen de $n$ diremos que la cadena de Marcov es \textbf{homogénea} en el tiempo.
\end{defi}
\begin{defi}
Denominamos \textbf{matriz de transición} $P$ de una cadena de Markov homogénea a la matriz dada por
$$
p_{ij} = P(X_{n+1}=j\mid X_{n}=i) 
$$
\end{defi}
\begin{defi}
Se dice que una matriz $P$ es una \textbf{matriz estocástica} si
	
	\begin{itemize}
		\item $p_{ij} \geq 0$ para todo $i,j$.
		\item Para cada fila $i$, $\sum_{j} p_{ij} =1$
	\end{itemize}
	
\end{defi}
\begin{prop}
Una cadena de Marcov homogénea viene completamente especificada por su matriz de transición $P$ y la distribución inicial sobre el espacio de estado.
\end{prop}
\begin{nota}
Si suponemos que nuestras cadenas son homogéneas podemos definir independientemente de $k$
$$p_{ij}^{(n)} = P(X_{n+k}=j\mid X_{k}=i)$$
 con $p_{ij}^{0}=1$ para todo $i = j$ y $p_{ij}=0$ en caso contrario. Entonces
\begin{align*}
p_{ij}^{(n)} &= \sum_{l=1}^{m}P(X_{n+k}=j, X_{r+k}=l\mid X_{k}=i)\\
&= \sum_{l=1}^{m} P(X_{n+k}=j\mid X_{r+k}=l)P(X_{r+k}=l\mid X_{k}=i)\\
&= \sum_{l=1}^{m}P_{il}^{r} P_{lj}^{n-r}
\end{align*}
con $r=0,\dotsc,n$. Se cumple, por tanto, que ${P}^{(n)} = {P}^{r} \cdot {P}^{n-r}$ con ${P}^{0}=I$.

\end{nota}
\begin{defi}
La \textbf{distribución marginal} viene dada por 
\begin{equation*}
\pi_{j}(n) = \sum_{i=1}^{m} P(X_{0}=i)P(X_{n}=j\mid X_{0}=i)=\sum_{i=1}^{m} \pi_{i}(0) P_{ij}^{(n)} \qquad  j=1,2,\dotsc 
\end{equation*}
Luego, $\pi(n)=\pi(0) P^{(n)}$.
\end{defi}

\section{Clasificación de estados}
\begin{defi}
Supongamos una cadena de Markov con $m$ estados y matriz de transición $P$. Diremos que un estado $E_i$ es \textbf{absorvente} si 
$$
p_{ii} = 1 \qquad p_{ij} = 0 \quad \forall j \neq i
$$
\end{defi}
\begin{defi}
Supongamos una cadena de Markov con $m$ estados y matriz de transición $P$. Diremos que un estado $E_i$ es \textbf{periódico} de periodo $t$ si 
$$
p_{ii}^{(n)} = 0  \quad p_{ij}^{(n)} = 0 \quad \forall t,2t,3t,\dotsc
$$
\end{defi}
\begin{defi}
Sea $d(i) = \gcd\{n\mid p_{ii}^{(n)}>0\}$. Entonces el estado $E_i$ es \textbf{periódico} si $d(i)>1$ y \textbf{aperiódico} si $d(i)=1$.
\end{defi}
\begin{example}
Supongamos una cadena de Markov con $4$ estados y matriz de transición
$$P = 
\begin{pmatrix}
0 & 1/2 & 0 & 1/2\\
0 & 0  & 1 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0
\end{pmatrix}
$$
Entonces es claro que
$$P^2 = 
\begin{pmatrix}
0 & 0 & 1 & 0\\
1 & 0  & 0 & 0\\
0 & 1/2 & 0 & 1/2 \\
1 & 0 & 0 & 0
\end{pmatrix} \qquad P^3= \begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 1/2  & 0 & 1/2\\
1 & 0 & 1 & 0\\
0 & 1/2 & 0 & 1/2
\end{pmatrix}
\qquad P^4 = P
$$
Se comprueba fácil,ente que todos los estados tienen período $3$.
\end{example}

\begin{defi}
Supongamos una cadena de Markov con $m$ estados y matriz de transición $P$. Diremos que un estado $E_j$ es \textbf{recurrente} si la probabilidad de que visite el estado $j$ es $1$. Cuando un estado no es recurrente se dice que es \textbf{transitorio}. 
\end{defi}
\begin{defi}
Denotamos por $f_{ij}$ la probabilidad de que partiendo de $i$ alcancemos $j$ en algún momento. Además, $f_{ij}^{(n)}$ denota la probabilidad de que la cadena visite por primera vez al estado $j$ en el $n$-ésimo instante partiendo de $i$. 
\end{defi}
\begin{prop}
Se tiene
$$
f_{ij} = \sum_{n=1}^\infty f_{ij}^{(n)}
$$
y además
$$
\begin{cases}
p_{ij}^{(1)} = f_{ij}^{(1)} \\
p_{ij}^{(n)} = \sum_{r=1}^{n}f_{ij}^{(r)}p_{jj}^{(n-r)}
\end{cases}
$$
\end{prop}
\begin{prop}
Una estado $E_i$ es recurrente si y solo si $f_i = 1$.
\end{prop}
\begin{defi}
Dada una v.a. discreta $X$ que toma valores enteros no negativos. Se define fgp a la función dada por
$$
P(s) = E[s^X] = \sum_{k=0}^\infty {s^k}p[X=k]
$$
\end{defi}
\begin{nota}
Podemos definir $p_{ij}^{(0)} = 1$ si $i=j$ y $0$ en caso contrario, y $f^{(0)}_{ij}=0$ en cualquier caso.
\end{nota}
\begin{theorem}
El estado $E_j$ es recurrente si y solo si
$$
\sum_{n = 0}^\infty p_{jj}^{(n)} = \infty
$$
Por tanto, será transitorio si y solo si la serie es convergente.
\end{theorem}

\begin{proof}
Podemos calcular las fg de $p_{ij}^{(n)}$ y $f_{ij}^{(n)}$, las cuales denotamos $P_{ij}(s)$ y $F_{ij}(s)$ respectivamente. Podemos escribir si $i\neq j$:
\begin{align*}
P_{ij}(s) &= \sum_{n=0}^\infty s^n p_{ij}^{(n)}\\
&= \sum_{n=0}^\infty s^n \left(\sum_{r=0}^{n}f_{ij}^{(r)}p_{jj}^{(n-r)}\right)\\
&=  \left(\sum_{n=0}^\infty s^n f_{ij}^{(n)}\right)\left( \sum_{n=0}^\infty s^n p_{jj}^{(n)}\right)\\
&=  F_{ij}(s)P_{jj}(s)
\end{align*}
Para $i=j$ el razonamiento es ligeramente distinto
\begin{align*}
P_{ii}(s) &= \sum_{n=0}^\infty s^n p_{ii}^{(n)}\\
&= 1+\sum_{n=0}^\infty s^n \left(\sum_{r=0}^{n}f_{ii}^{(r)}p_{ii}^{(n-r)}\right)\\
&= 1+ \left(\sum_{n=0}^\infty s^n f_{ij}^{(n)}\right)\left( \sum_{n=0}^\infty s^n p_{jj}^{(n)}\right)\\
&=  1 + F_{ij}(s)P_{jj}(s)
\end{align*}
Despejando, obtenemos
\begin{align*}
P_{ii}(s) &= \frac{1}{1-F_{ii}(s)} 
\end{align*}
Por tanto, $$P_{jj}(1)=\infty \Leftrightarrow  F_{jj}(1)=1 \Leftrightarrow f_j = 1 \Leftrightarrow \text{$E_j$ es recurrente}$$ .
\end{proof}

\begin{example}
Supognamos una cadena de Markov con $3$ estados y matriz de transición
$$
P=
\begin{pmatrix}
p & 1-p &0 \\
0 & 0 & 1\\
1-q & 0 & q
\end{pmatrix}
$$
donde $0<p,q<1$. Demostrar que $E_1$ es recurrente. Es fácil comprobar que
\begin{align*}
f_{11}^{(1)} &= p\\
f_{11}^{(2)} &=0\\
 f_{11}^{(n)}&=(1-p)(1-q)q^{n-3} \quad \forall n\geq 3\\
\end{align*}
Por tanto
$$
F_{11}(1) = p + (1-p)(1-q)\sum_{n=3}^\infty q^{n-3} = 1
$$
Por tanto, $P_{11}(1)=\infty$ y el proceso es recurrente por el teorema anterior.
\end{example}
\begin{defi}
Se define el \textbf{tiempo medio de recurrencia} de un estado recurrente $E_j$ al valor cuando exista de
$$
\mu_j=  \sum_{n=1}^\infty nf_j^{(n)}
$$
Un estado recurrente $E_j$ se dice que es \textbf{nulo} si $\mu_j=\infty$ y \textbf{no nulo} en caso contrario.
\end{defi}
\begin{theorem}
Un estado recurrente $E_j$ es nulo si y solo si $p_{jj}^{(n)} \to 0$. Además esto ocurre si $p_{ij}^{(n)} \to 0$ para todo $i$.
\end{theorem}
\begin{proof}
Recordemos que
$$
P_{jj}(s)=\frac{1}{1-F_{jj}(s)} \Rightarrow F_{jj}(s)=1-\frac{1}{P_{jj}(s)} 
$$
Un estado $j$ es recurrente si $P_{jj}(1) = \infty$ o $F_{jj}(1)=1$. Además, notemos que $F'_{jj}(1)=\mu_j$. Vamos aplicar el teorema de Cesaro. Éste establece que, sea una sucesión $a_k$ con límite $a$, entonces
$$
\lim_{n\to \infty} \frac{1}{n+1}\sum_{k=0}^n a_k = a
$$
Es más,
$$
\lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k = \lim_{s\to 1}(1-s)\sum_{n=0}^\infty a_n s^n
$$
Dado que $F_{jj}(1)=1$, podemos aplicar L'Hopital en la siguiente igualdad
$$
\lim_{s\to 1} (1-s)P_{jj}(s)=\lim_{s \to 1} \frac{1-s}{1-F_{jj}(s)} =  \lim_{s \to 1} \frac{1}{F_{jj}'(s)}  = \frac{1}{\mu_j}
$$
Por tanto, si existe $\lim_{n\to \infty} p_{jj}^{(n)}$ entonces vale $1/\mu_j$. 
\begin{itemize}
\item Supongamos que $p_{jj}^{(n)}\to0$. Entonces $\mu_j = \infty$ y por tanto el proceso es nulo.
\item Supongamos que $\mu_j = 0$, entonces si existe el límite de $p_{jj}^{(n)}$ este es $0$.
\end{itemize}
Es más, usando que $P_{ij}(s)=F_{ij}(s)P_{jj}(s)$ tenemos que para cualquier $i\neq j$
$$
\frac{1}{\mu_j} = \lim_{s\to 1} (1-s)P_{jj}(s)= \lim_{s\to 1} (1-s)\frac{P_{ij}(s)}{F_{ij}(s)} = \lim_{s\to 1} (1-s)\frac{P_{ij}(s)}{F_{ij}(1)} 
$$
Por tanto, si existe el límite de $p_{ij}^{(n)}$ este debe ser $F_{ij}(1)/m_j$. Si $m_j = \infty$ entonces todos los $p_{ij}^{(n)} \to 0$.
\end{proof}

\newpage
\begin{defi}
Un estado $E_j$ se dice que \textbf{ergódico} si es recurrente, no nulo y aperiódico.
\end{defi}

\section{Clasificación de las cadenas}
\begin{defi}
Dados los estados $i$ y $j$, se dice que \textbf{$i$ se comunica con $j$} (denotado $i\to j$ si existe $nºgeq 1$ tal que $p_{ij}^{(n)}>0$. Si $\to j$ y $j\to i$, se dice que \textbf{$i$ se intercomunica con $j$}  y se denota por i$\leftrightarrow j$.
\end{defi}
\begin{prop}
Si $i\leftrightarrow j$, entonces ambos son simultáneamente recurrentes o transitorios.
\end{prop}
\begin{proof}
Tomemos $m,n$ tales que $p^{(m)}_{ij},p^{(n)}_{ji}>0$. Definimos $\alpha =p^{(m)}_{ij}p^{(n)}_{ji}$ y sea $k$ un entero mayor o igual que $0$. Se tiene que 
$$
p_{ii}^{(m+k+n)} \geq p^{(m)}_{ij}p_{jj}^{(k)}p^{(n)}_{ji}\geq \alpha p_{jj}^{(k)}
$$
Con nun razonamiento análogo, $p_{jj}^{(m+k+n)}\geq \alpha p_{ii}^{(k)}$. Por tanto, las series que definen tienen el mismo carácter.
\end{proof}
\begin{prop}
Si $i\leftrightarrow j$ entonces $d(i)=d(j)$.
\end{prop}
\begin{proof}
Supongamos que se tiene la intercomunicación. Tomemos $m,n$ tales que $p^{(m)}_{ij},p^{(n)}_{ji}>0$. Sea $R(i) = \{k\in \mathbb{N} \mid p_{ii}^{(k)}>0\}$. Igualmente $R(j)$. Sea entonces $d(i)=\gcd R(i)$. Como $p_{jj}^{(m+n)}>0$ entonces $m+n \in R(j)$. Por tanto, $d(j)\mid m+n$.

Si $k\in R(i)$ entonces $m+k+n \in R(j)$, y por tanto $d(j)\mid k+m+n$ y por tanto $d(j)\mid k$. Por tanto, $d(j)$ divide a todos los elementos de $R(i)$. Se sigue que $d(j) \leq d(i)$. Análogamente se tiene la otra desigualdad.
\end{proof}
\begin{prop}
Si $i\leftrightarrow j$ y el estado $i$ es recurrente nulo, entonces el estado $j$ es recurrente nulo.
\end{prop}
\begin{proof}
Supongamos que se tiene la intercomunicación. Tomemos $m,n$ tales que $p^{(m)}_{ij},p^{(n)}_{ji}>0$. Sea $k$ entero mayor que $0$. Entonces se tiene que
$$
p_{ii}^{(n+m+k)} \geq \alpha p_{jj}^{(k)}\geq 0
$$
Si hacemos $k\to \infty$ entonces $p_{ii}^{(n+m+k)} \to 0$, luego $\alpha p_{jj}^{(k)} \to 0$ y, en particular, $p_{jj}^{(k)} \to 0$.
\end{proof}
\begin{defi}
Un subconjunto $C\subset E$ se dice que es \textbf{irreducible} si todos los estados están intercomunicados. Si $E$ es irreducible, la cadena se dice irreducible.
\end{defi}
\begin{nota}
Nótese que, por el resultado anterior, si una cadena es periódica entonces todos los estados tienen igual periodo.
\end{nota}
\begin{defi}
Una cadena se dice \textbf{aperiódica} si todos los estados tienen sus estados aperiódicos. Una cadena irreducible que tiene todos sus estados recurrentes positivos y aperiódicos se dice que es una cadena ergódica.
\end{defi}
\begin{defi}
Un subconjunto $C\subset E$ se dice que es \textbf{cerrado} $p_{ij}=0$ para todos $i\in C$, $j\not\in C$.
\end{defi}
\begin{theorem}
El conjunto $E$ de estados de una cadena de Markov se puede particionar de manera única como
$$
E=T\cup C_1\cup C_2 \cup \dotsc
$$
donde $T$ es el conjunto de estados transitorios de la cadena y $C_1,C_2,\dotsc$ son conjuntos irreducibles y cerrados de estados recurrentes.
\end{theorem}
\begin{nota}
Si la cadena entra en una clase cerrada e irreducible nunca saldrá de ella. Si la cadena entra en una clase de estados transitorios entonces se queda ahí permanentemente o entra en una clase cerrada e e irreducible de estados recurrentes. Eventualmente el conjunto de estados transitorios puede ser vacío.
\end{nota}
\begin{theorem}
Si una cadena de Markov $S$ es finita entonces todos los estados no pueden ser transitorios, siendo los estados recurrentes no nulos.
\end{theorem}
\begin{example}
Supongamos una cadena de Markov con $6$ estados y matriz de transición dada por 
$$
P=
\begin{pmatrix}
1/2 & 1/2 & 0 & 0 & 0 & 0\\
3/4 & 1/4 & 0 & 0 & 0 & 0\\
1/4 & 1/4 & 1/4 & 1/4 & 0 & 0\\
1/4 & 0 & 1/4 & 1/4 & 0 & 1/4\\
0 & 0 & 0 &0 & 1/2 & 1/2\\
0 & 0 & 0 &0 & 1/2 & 1/2
\end{pmatrix}
$$
Tenemos entonces que obviamente $\{1,2\}$ y $\{5,6\}$ son cerrados e irreducibles, mientras que el resto son transitorios.
\end{example}
\section{Distribuciones estacionarias}
\begin{defi}
Sea $S$ una cadena de Markov con matriz de transición $P$. Una \textbf{distribución límite} de una cadena de Markov es una distribución de probabilidad $\lambda$ tal que para todo $i,j$ se tiene que
$$
\lim_{n\to \infty} p_{ij}^{(n)}=\lambda_j
$$
\end{defi}
\begin{prop}
Para cualquier distribución inicial, y pra todo $J$
$$
\lim_{n\to \infty} P[X_n=j]=\lambda_j
$$
Para cualquier distribución inicial $\alpha$
$$
\lim_{n\to \infty} \alpha P^n = \lambda
$$
Además
$$
\lim_{n\to \infty}  P^n = \Lambda
$$
donde $\Lambda$ es una matriz estocástica por filas.
\end{prop}
\begin{defi}
Sea $S$ una cadena de Markov con matriz de transición $P$. Una distribución de probabilidad $\pi$ se dice que \textbf{estacionaria} si 
$$
\pi = \pi P
$$
\end{defi}
\begin{nota}
La distribución estacionaria ni es única ni existe siempre.
\end{nota}
\begin{prop}
Sea $S$ una cadena de Markov con matriz de transición $P$. Si $\pi$ es la distribución límite entonces es estacionaria.
\end{prop}
\begin{proof}
$$
\pi = \lim \alpha P^{n} = \lim \alpha P^{n-1} P = \pi P
$$
\end{proof}
\begin{defi}
Una matriz $M$ se dice que es \textbf{positiva} si todos los valores $m_{ij}$ de $M$ son positivos. Se denota por $M>0$. Análogamente para vectores.
\end{defi}
\begin{defi} Una matriz de transición $P$ se dice que es \textbf{regular} si para alguna potencia $n\geq 1$ de $P$ se tiene que $P^n>0$.
\end{defi}
\begin{theorem}
Una cadena de Markov es ergódica si, y solo si, la matriz de transición es regular.
\end{theorem}
\begin{theorem}
Sea $S$ una cadena de Markov con matriz de transición $P$. Si $P$ es regular entonces tiene distribución límite, la cual es única, positiva y además es la distribución estacionaria.
\end{theorem}
\begin{prop}
Sea $\pi$ una distribución estacionaria para una cadena de Markov. Si $j$ es un estado transitorio o recurrente nulo entonces $\pi_j=0$.
\end{prop}
\begin{theorem}
Sea $S$ una cadena de Markov finita irreducible con matriz de transición $P$. Entonces, $\mu_j$ es finito, y existe una única distribución estacionaria positiva $\pi$ tal que
$$
\pi_j = \frac{1}{\mu_j}
$$
Más aún, se cumple que
$$
\pi_j = \lim_{n\to \infty} \frac{1}{n}\sum_{m=0}^{n-1}p_{ij}^{(m)}
$$
\end{theorem}
\begin{example}
Si tenemos la cadena de Markov dada por la matriz de transición
$$
P = 
\begin{pmatrix}
1 & 0 & 0 \\
1/3 & 1/3 & 1/3\\
0 & 0 & 1
\end{pmatrix}
$$
Es claro que no existe una única distribución límite.
\end{example}
\end{document}
