\documentclass[AL.tex]{subfiles}

\begin{document}


%\hyphenation{equi-va-len-cia}\hyphenation{pro-pie-dad}\hyphenation{res-pec-ti-va-men-te}\hyphenation{sub-es-pa-cio}

\chapter{Análisis asintótico}

\section{Complejidad}

El análisis asintótico consiste en el estudio de la eficiencia de un algoritmo en el \emph{peor caso}. 

\begin{ej}
Supongamos que tenemos un inventario, que para poder leerlo necesitamso 10000 milisegundos (ms) y 10 ms en procesar una transacción (por ejemplo clasificar un libro). Entonces, el tiempo de $n$ transacciones será $T(n)=10000+10n$. Cuando $n>>0$, el término que domina es $10n$. 
\end{ej}

La notación que usamos es la llamada Big-o (o grande): si $n$ es el tamaño de la entrada, $T(n)$ es una función de complejidad temporal (running time) y supongamos que existe una función conocida $f(n)$, decimos que $T(n)\in O(f(n))$ si $T(n)\leq c f(n)$ para alguna constante $c$ a partir de un $n$ suficientemente grande (se dice que $T$ es de orden $f(n)$). $O(f(n))$ es el conjunto de funciones que cumplen la propiedad que acabamos de definir.

\begin{ej}
$T(n)=10^6n\in O(n)$. Podemos tomar $C=10^6$ y $n_0=1$, de modo que para todo $n\geq n_0$ se cumple la definición. Se observa con este ejemplo que la notación $O(\cdot)$ prescinde de las constantes. Se tiene además que, por ejemplo.
\end{ej}
\begin{ej}
$T(n)=n\in O(n^k)$ para todo $k\geq 1$. $T(n)=n^3+n^2+n\in O(n^3)$, por ejemplo con $c=3$ y $n_0=1$.
\end{ej}

\begin{ej}
$n^2\notin O(n)$. 
\end{ej}

\begin{ej}
$O(\cdot)$ no da toda la información. El algoritmo de búsqueda binaria tiene una función de complejidad temporal $T(n)=\log n\in O(\log n)$. Pero este es el peor caso. En el mejor caso podría encontrar el elemento al primer paso, es decir, sería $O(1)$. Exite también el estudio del caso promedio, aunque no lo estudiaremos. 
\end{ej}

\begin{ej}
$e^{3}\notin O (e^n)$ pues $e^{3n}=e^{2n}e^n$, luego no se puede minorar por una constante (se puede probar usando límites). $10^n\notin O(2^n)$ por una razón similar. 
\end{ej}

\begin{ej}
Consideremos $T(n)=n\log n$ y $U(n)=100n$. Asintóticamente $U(n)$ es más rápido, pero el $n_0$ a partir del cual se cumple esto es muy grande para la mayoría de los casos prácticos, en los que $\log n\sim 50$. Así que en la práctica es preferible $T(n)$.
\end{ej}

Introducimos la notación $\Omega(f(n))$ para referirnos al conjunto de las funciones $T(n)$ tales que $T(n)\geq Df(n)$ para todo $n\geq n_0$, $D\in\N$. Es claro que $\Omega$ es inverso de $O$, es decir, $T(n)\in O(f(n))\Leftrightarrow f(n)\in\Omega(T(n))$.

\begin{ej}
$2n\in\Omega(n)$. Se puede ver por la definición o por la equivalencia con $O$. De la misma forma, $n^2\in\Omega(3n^2+n\log n)$. 
\end{ej}

La notación $\Theta(f(n))$ se refiere al conjunto $O(f(n))\cap\Omega(f(n))$. En cierto sentido, $\Theta$ representa la complejidad ``óptima''.

\begin{ej}
$n^3\in\Theta(3n^3-n^2)$. 
\end{ej}

\begin{ej}
El algoritmo para calcular el máximo de $n$ valores tiene $T(n)=\Theta(n)$. De hecho es lineal siempre, puesto que siempre hay que comprobar todos los elementos. 
\end{ej}

\begin{ej}
$T(n)=n(1+\sin n)\in O(n)$ y $T(n)\in\Omega(0)$ ya que $\sin n\in [-1,1]$. No puede estar en $\Theta(f(n))$, porque la gráfica de la función oscila entre $2n$ y 0. Y no puede ser otra función porque tiene que estar entre esas 2. 
\end{ej}

\begin{nota}
Asumimos el modelo de computación RAM, donde hay operaciones de coste constante $O(1)$: $+,-,\times, \div, \leq$, if... llamadas funciones primitivas. Hay también operaciones más complejas como bucles, subrutinas, etc. que sí dependerán de $n$. En la práctica esto no es exactamente así, pues no se tarda lo mismo en sumar dos números pequeños que dos números muy grandes. 
\end{nota}

\subsection{Complejidades habituales}

$O(1)\subset O(\log n)\subset O(n)\subset O(n\log n)\subset O(n^2)\subset O(2^n)\subset O(n!)\subset O(n^n)$

Normalmente se considerar que un algoritmo es eficiente cuando a lo sumo está en $O(n\log n)$. Si está en $O(n^k)$ para $k\geq 2$ se llama polinomial. A partir de $O(2^n)$ se considera inútil.


\section{Ejercicios}

\begin{ejer}[La menor caja contenedora]\

Input: $n$ puntos en $\R^p$. 

Output: la caja $p$-dimensional de mínimo volumen que contiene a todos los puntos. 

Una caja es hipercubo que tiene lados paralelos a los ejes (isotéticos). 

El ejercicio consiste en dar un algoritmo (preferiblemente no inútil) y su complejidad.

%dbanez@us.es
\end{ejer}

\section{Análisis de algoritmos}

\subsection{Problema 1}
Dado un conjunto de items, decidir si hay o no dos iguales. Existe un algoritmo sencillo que es cuadrático, ya que hay $\binom{n}{2}$ pares, y se trata de comparar los elementos de los pares. Otra posibilidad es ordenar la lista, lo cual ya nos detectaría si hay dos iguales ya sea en el propio ordenamiento o bien luego mirando dos seguidos. Así que podemos mejorar la eficiencia si encontrar una forma de ordenar de complejidad menor que cuadrática, el cual existe (en concreto $O(n\log n)$).

\subsection{Problema 2}
Dados $m$ hombres y $w$ mujeres, ver qué pareja $(w,m)$ es compatible, suponiendo que comprobar la compatibilidad es de complejidad constante. Podría darse una descripción menos sexista, pero esta es la que ha dado el profesor. El algoritmo exhaustivo consistiría en realizar todas las parejas posibles y comprobar si son compatibles, que tiene orden $O(wm)$. Obsérvese que $w$ y $m$ son variables independientes, por lo que $O(wm)\neq O(w^2)$. 


A propósito de este último comentario, nos preguntamos si $\Theta(w^6+wm+m^3)=\Theta(w^6+m^3)$. Si $w>>m$, entonces $w^6+wm+m^3$ estará en la misma clase que $w^6+m^3$, que es $\in\Theta(w^6)$. Si $w<<m$ podemos hacer el razonamiento análogo, con lo que pertenecería a $\Theta(m^3)$. Así que la respuesta es afirmativa. Se puede probar haciendo el cociente y tomando límtie, que da 1 y por tanto efectivamente son del mismo orden. 

Sin embargo $\Theta(w^6+wm)\neq\Theta(w^6)$, porque si domina $m$ no se tiene la igualdad. 

\subsection{Problema 3}
Hay un arreglo ordenado por título de $n$ CD's. Existe $k$ que comienzan por ``The best of...''. Se trata de obtener todos los de este tipo. Hacemos búsqueda binaria. En el mejor caso llegas a un disco de ellos y avanzas y retrocedes para coger los todos, con lo que sería $O(k)$. En el peor caso hay $O(\log n)$ pasos y luego $O(k)$, o sea, $O(\log n +k)$. Este es un algoritmo \emph{output sensitive}, porque dependiendo del tamaño de la salida tiene una complejidad. 


\subsection{Problema 4}
Ordenar un conjunto $L$ de items. Vamos a ver el algoritmo Merge-Sort. 
\begin{enumerate}
\item Se divide $L$ en dos sublistas $L_1$ y $L_2$ de igual (lo más igual posible) tamaño. $T(n)=O(1)$
\item Se ordena cada sublista con llamadas recursivas. $T(n)=2T(\frac{n}{2})$
\item Se mezclan $L_1$ y $L_2$.  Para ello se comparan los mínimos y se elimina el que se ha elegido. $T(n)=O(n)$
\end{enumerate}

Algoritmo recursivo con 
\[
T(n)=\begin{cases}
O(1) & n\leq 2\\
2T(\frac{n}{2})+\Theta(n) & n>2
\end{cases}
\]
Tenemos que resolver la ecuación de recursión $T(n)=2T(\frac{n}{2})+\Theta(n)$. Para ello hacemos lo siguiente
\[
2T(\frac{n}{2})+\Theta(n)=2(2T(\frac{n}{2^2}+\Theta(\frac{n}{2}))+\Theta(n)=2^2T(\frac{n}{2^2})+2\Theta(n)
\]
Seguimos aplicando esta estrategia y obtenemos $T(n)=2^kT(\frac{n}{2^k})+k\Theta(n)$. Cuando $n=2^k\Rightarrow k\in O(\log n)$ se termina, obteniendo $T(n)=nT(1)+O(\log n)\Theta(n)=O(n)+O(n\log n)=O(n\log n)$. 

Vemos ahora un teorema general que nos ayudará a resolver ecuaciones de recurrencia como la anterior.

\begin{teorema}[Teorema del maestro]

Si $T(n)=\begin{cases}
O(1) & n\leq n_0\\
aT(\frac{n}{b})+\Theta(f(n)) & n> n_0
\end{cases}$, entonces
\begin{enumerate}
\item Si $f(n)\in O(n^{\log_ba-\varepsilon})$, $\varepsilon\in [0,1]$, entonces $T(n)\in\Theta(n^{\log_ba})$.
\item Si $f(n)\in \Theta(n^{\log_ba})$, entonces $T(n)\in\Theta(n^{\log_ba}\log n)$.
\item Si $f(n)\in\Omega(n^{\log_ba+\varepsilon})$, $\varepsilon\in[0,1]$, y $af(\frac{n}{b})\leq cf(n)$ para $c<1$ y $n$ suficientemenre grande, entonces $T(n)\in\Theta(f(n))$. 
\end{enumerate}

\end{teorema}
En el enunciado del teorema, $a$ representa el número de llamadas recursivas, $b$ las partes en las que se divide el conjunto y $f(n)$ es el tiempo necesario para ``procesado extra''. La prueba del resultado puede encontrarse en el libro de Cormen A LO MEJOR PONER AL PRINCIPIO QUE TODAS LAS DEMOSTRACIONES ESTÁN AHÍ. En dicha prueba se muestra además que el costo de todas las llamadas recursivas es $\Theta(n^{\log_ba})$ y el costo del procesado extra es $\Theta(f(n))$. En función de cuál de estos costos domine más, obtenemos cada uno de los casos del teorema. 

\begin{ejs}\
\begin{enumerate}
\item En el caso de Merge-Sort, $a=b=2$, con lo que $\log_22=1$. En este caso $f(n)=\Theta(n)$ (abuso de lenguaje), que coincide con el caso 2 del teorema. Así que $T(n)\in\Theta(n\log n)$, que fue lo que demostramos a mano. 

\item La búsqueda binaria tiene una recurrencia $T(n)=T(\frac{n}{2})+O(1)$, puesto que se divide la lista en 2 tras hacer una pregunta para decidir con qué mitad quedarnos. Así que $a=1$, $b=2$, con lo que $\log_21=0$ y $f(n)=O(1)\in O(n^0)$. Volvemos a estar en el caso 2 del teorema, con lo que $T(n)=\Theta(\log n)$, cosa que hay conocíamos. 

\item Resolvemos $T(n)=4T(\frac{n}{2})+n^3$. Claramente $a=4$, $b=2$ y $f(n)=n^3$, de donde $\log_24=2$. Como $n^3\in\Omega (n^{2+\varepsilon})$ para cualquier $\varepsilon\leq 1$ y $4(\frac{n^3}{8})=\frac{1}{2}n^3$, estamos en las hipótesis del caso 3 del teorema. Por ello concluimos que $T(n)\in\Theta(n^3)$. 

\item $T(n)=2T(\frac{n}{2})+n^2$. Tenemos aquí $a=2$, $b=2$ y $f(n)=n^2$. De aquí $\log_22=1$, y como $n^2\in\Omega(n^{1+\varepsilon})$ para cualquier $\varepsilon\leq 1$, estmos en el tercer caso. Esto nos da $T(n)\in\Theta(n^2)$. REVISAR 
\end{enumerate}
\end{ejs}

\begin{coro}[versión para $f(n)=\Theta(n^k)$]
En las condiciones del teorema
\begin{enumerate}
\item Si $k<\log_ba$, entones $T(n)\in\Theta(n^{\log_ba})$.
\item Si $k=\log_ba$, entonces $T(n)\in\Theta(n^{\log_ba}\log n)$.
\item Si $k>\log_ba$, entonces $T(n)\in\Theta(n^k)$. 
\end{enumerate}
\end{coro}

Atención, el teorema del maestro no resuelve todos los casos, como vemos a continuación.
\begin{ej}
Sea la ecuación de recurrencia $T(n)=2T(\frac{n}{2})+\Theta(n\log n)$. Aquí $a=b=2$ y $f(n)=n\log n$. Aquí $f$ no verifica ninguno de los dos casos. El más interesante de ver es el tercero. $n^{1+\varepsilon}=nn^{\varepsilon}$. Las $n$ se compensan, luego tendríamos que comparar $n^{\varepsilon}$ con $\log n$. Se puede demostrar que $\frac{n^{\varepsilon}}{\log n}\to\infty$ cuando $n\to\infty$, por lo que efectivamente $n^{1+\varepsilon}\notin O(n\log n)$. Como $f$ no está en ninguno de los casos, el teorema del maestro no es aplicable. Sin embargo sí lo será el siguiente teorema.
\end{ej}

\begin{teorema}
Si $(n)=aT(\frac{n}{b})+\Theta(n^{\log_ba}\log^kn)$, entonces $T(n)\in\Theta(n^{\log_ba}\log^{k+1}n)$.
\end{teorema}

En el ejemplo anterior podemos aplicar este teorema para $k=1$, con lo que $T(n)\in\Theta(n\log^2n)$. 

\end{document}
